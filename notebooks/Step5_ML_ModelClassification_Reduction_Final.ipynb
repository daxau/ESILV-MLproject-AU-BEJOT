{
 "cells": [
  {
   "cell_type": "raw",
   "id": "db24864f-ff4f-42c9-bafd-cb780aa055da",
   "metadata": {},
   "source": [
    "# ===============================================================\n",
    "# MACHINE LEARNING PROJECT - 2025 GUIDELINES COMPLIANT\n",
    "# Title: Stock Price Direction Forecasting using Macroeconomic Indicators\n",
    "# Author:\n",
    "# ===============================================================\n",
    "\n",
    "# Predicting Apple Stock Direction — Full ML Pipeline\n",
    "# This notebook follows the full structured process:\n",
    "\n",
    "1) Import libraries & dataset\n",
    "2) Create binary target (Apple_Direction)\n",
    "3) Data exploration & preprocessing\n",
    "4) Train/test split\n",
    "5) Train models\n",
    "6) Hyperparameter tuning\n",
    "7) Evaluate metrics & compare models\n",
    "8) PCA and overfitting checks\n",
    "9) Final comparison & conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f0694cd-7df9-4cab-950a-a305dc9b4b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 1) IMPORT LIBRARIES\n",
    "# ===============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report, RocCurveDisplay\n",
    "import shap\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e939bfd-5e2f-4da7-b5ca-4fac503faaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dax_a\\ml_outputs\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# 1) LOAD DATASETS\n",
    "# ===============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "OUT = Path.home() / \"ml_outputs\"\n",
    "print(OUT)\n",
    "\n",
    "data = pd.read_csv(OUT / 'Cleaned_Features_for_ML.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "850bbc61-2f57-45a9-85f1-f710d7c297cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset shape: (3970, 24)\n",
      "Columns kept: ['US10Y', 'LQD', 'HangSeng', 'IEF', 'DowJones', 'BND', 'TLT', 'Nikkei225', 'MSCIWorld', 'MA20', 'Imports_GDP_Pct', 'US2Y', 'Momentum', 'S&P500', 'Recession_Probability', 'Exports_GDP_Pct', 'DAX', 'Inflation_Annual_Pct', 'Fed_Funds_Rate', 'CAC40', 'Apple', 'Return', 'Direction', 'target_index']\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# STEP — REDUCE DATASET TO TOP 20 FEATURES + REQUIRED COLUMNS\n",
    "# ===============================================================\n",
    "\n",
    "# Top 20 selected features\n",
    "top20_features = [\n",
    "    \"US10Y\", \"LQD\", \"HangSeng\", \"IEF\", \"DowJones\", \"BND\", \"TLT\",\n",
    "    \"Nikkei225\", \"MSCIWorld\", \"MA20\", \"Imports_GDP_Pct\", \"US2Y\",\n",
    "    \"Momentum\", \"S&P500\", \"Recession_Probability\", \"Exports_GDP_Pct\",\n",
    "    \"DAX\", \"Inflation_Annual_Pct\", \"Fed_Funds_Rate\", \"CAC40\"\n",
    "]\n",
    "\n",
    "\n",
    "# Columns that MUST remain in the dataset\n",
    "mandatory_columns = [\"Apple\", \"Return\", \"Direction\", \"target_index\"]\n",
    "\n",
    "# Make sure they exist\n",
    "for col in mandatory_columns:\n",
    "    if col not in data.columns:\n",
    "        print(f\"Warning: column '{col}' was not found in data.\")\n",
    "\n",
    "# Build the new reduced dataframe\n",
    "keep_columns = top20_features + mandatory_columns\n",
    "data = data[keep_columns].copy()\n",
    "\n",
    "print(\"New dataset shape:\", data.shape)\n",
    "print(\"Columns kept:\", data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c954304-41a2-4855-9624-61165e5ddcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3970 entries, 2010-07-29 to 2025-11-03\n",
      "Data columns (total 24 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   US10Y                  3970 non-null   float64\n",
      " 1   LQD                    3970 non-null   float64\n",
      " 2   HangSeng               3970 non-null   float64\n",
      " 3   IEF                    3970 non-null   float64\n",
      " 4   DowJones               3970 non-null   float64\n",
      " 5   BND                    3970 non-null   float64\n",
      " 6   TLT                    3970 non-null   float64\n",
      " 7   Nikkei225              3970 non-null   float64\n",
      " 8   MSCIWorld              3970 non-null   float64\n",
      " 9   MA20                   3970 non-null   float64\n",
      " 10  Imports_GDP_Pct        3970 non-null   float64\n",
      " 11  US2Y                   3970 non-null   float64\n",
      " 12  Momentum               3970 non-null   float64\n",
      " 13  S&P500                 3970 non-null   float64\n",
      " 14  Recession_Probability  3970 non-null   float64\n",
      " 15  Exports_GDP_Pct        3970 non-null   float64\n",
      " 16  DAX                    3970 non-null   float64\n",
      " 17  Inflation_Annual_Pct   3970 non-null   float64\n",
      " 18  Fed_Funds_Rate         3970 non-null   float64\n",
      " 19  CAC40                  3970 non-null   float64\n",
      " 20  Apple                  3970 non-null   float64\n",
      " 21  Return                 3970 non-null   float64\n",
      " 22  Direction              3970 non-null   int64  \n",
      " 23  target_index           3970 non-null   float64\n",
      "dtypes: float64(23), int64(1)\n",
      "memory usage: 775.4 KB\n",
      "None\n",
      "               US10Y       LQD  HangSeng       IEF  DowJones       BND  \\\n",
      "Date                                                                     \n",
      "2010-07-29 -0.000666  0.000364  0.000125  0.002094 -0.002926  0.001102   \n",
      "2010-07-30 -0.030677  0.004645 -0.003035  0.005850 -0.000117  0.001468   \n",
      "2010-08-02  0.019264 -0.002120  0.018211 -0.003092  0.019916 -0.000220   \n",
      "2010-08-03 -0.016537  0.002463  0.002096  0.004700 -0.003560  0.002696   \n",
      "2010-08-04  0.013040 -0.003094  0.004298 -0.004159  0.004141 -0.001955   \n",
      "2010-08-05 -0.012534  0.001826  0.000085  0.004802 -0.000510  0.001592   \n",
      "2010-08-06 -0.031218  0.005740  0.005897  0.006546 -0.002007  0.002934   \n",
      "2010-08-09 -0.000708  0.003261  0.005664 -0.000206  0.004242 -0.001219   \n",
      "2010-08-10 -0.014529 -0.001264 -0.015044  0.005058 -0.005094  0.002075   \n",
      "2010-08-11 -0.034520 -0.000090 -0.008339  0.005650 -0.024936  0.000731   \n",
      "\n",
      "                 TLT  Nikkei225  MSCIWorld      MA20  ...  \\\n",
      "Date                                                  ...   \n",
      "2010-07-29 -0.000304  -0.005870        0.0  0.002010  ...   \n",
      "2010-07-30  0.015565  -0.016370        0.0  0.002153  ...   \n",
      "2010-08-02 -0.014038   0.003461        0.0  0.002705  ...   \n",
      "2010-08-03  0.005772   0.012925        0.0  0.002720  ...   \n",
      "2010-08-04 -0.007653  -0.021113        0.0  0.000901  ...   \n",
      "2010-08-05  0.004668   0.017344        0.0  0.000770  ...   \n",
      "2010-08-06  0.010906  -0.001222        0.0  0.000166  ...   \n",
      "2010-08-09 -0.003696  -0.007221        0.0  0.000934  ...   \n",
      "2010-08-10  0.002105  -0.002240        0.0  0.001554  ...   \n",
      "2010-08-11  0.013409  -0.027034        0.0 -0.000408  ...   \n",
      "\n",
      "            Recession_Probability  Exports_GDP_Pct       DAX  \\\n",
      "Date                                                           \n",
      "2010-07-29                   0.06        12.341355 -0.007160   \n",
      "2010-07-30                   0.06        12.341355  0.002163   \n",
      "2010-08-02                   0.04        12.341355  0.023448   \n",
      "2010-08-03                   0.04        12.341355  0.002508   \n",
      "2010-08-04                   0.04        12.341355  0.003713   \n",
      "2010-08-05                   0.04        12.341355  0.000355   \n",
      "2010-08-06                   0.04        12.341355 -0.011676   \n",
      "2010-08-09                   0.04        12.341355  0.014693   \n",
      "2010-08-10                   0.04        12.341355 -0.010289   \n",
      "2010-08-11                   0.04        12.341355 -0.021027   \n",
      "\n",
      "            Inflation_Annual_Pct  Fed_Funds_Rate     CAC40     Apple  \\\n",
      "Date                                                                   \n",
      "2010-07-29              1.640043            0.18 -0.005027 -0.010922   \n",
      "2010-07-30              1.640043            0.18 -0.002401 -0.003332   \n",
      "2010-08-02              1.640043            0.19  0.029889  0.017881   \n",
      "2010-08-03              1.640043            0.19 -0.001205  0.000306   \n",
      "2010-08-04              1.640043            0.19  0.003525  0.004009   \n",
      "2010-08-05              1.640043            0.19  0.000923 -0.004867   \n",
      "2010-08-06              1.640043            0.19 -0.012789 -0.006152   \n",
      "2010-08-09              1.640043            0.19  0.016501  0.006383   \n",
      "2010-08-10              1.640043            0.19 -0.012387 -0.008940   \n",
      "2010-08-11              1.640043            0.19 -0.027419 -0.035542   \n",
      "\n",
      "              Return  Direction  target_index  \n",
      "Date                                           \n",
      "2010-07-29 -0.010922          0     -0.010922  \n",
      "2010-07-30 -0.003332          1     -0.003332  \n",
      "2010-08-02  0.017881          1      0.017881  \n",
      "2010-08-03  0.000306          1      0.000306  \n",
      "2010-08-04  0.004009          0      0.004009  \n",
      "2010-08-05 -0.004867          0     -0.004867  \n",
      "2010-08-06 -0.006152          1     -0.006152  \n",
      "2010-08-09  0.006383          0      0.006383  \n",
      "2010-08-10 -0.008940          0     -0.008940  \n",
      "2010-08-11 -0.035542          1     -0.035542  \n",
      "\n",
      "[10 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data.info())\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3289a7c5-76e5-4d1f-9496-37ef1194b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# SUPPRESS ARIMA WARNINGS SAFELY\n",
    "# ===============================================================\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ValueWarning)\n",
    "\n",
    "# ===============================================================\n",
    "# ARIMA MODEL\n",
    "# ===============================================================\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "data = data.sort_index()\n",
    "\n",
    "model = ARIMA(data[\"Return\"], order=(2,1,2))\n",
    "model_fit = model.fit()\n",
    "\n",
    "forecast = model_fit.predict(start=1, end=len(data), dynamic=False)\n",
    "forecast.index = data.index\n",
    "\n",
    "data[\"ARIMA_Return_Forecast\"] = forecast.shift(1)\n",
    "data = data.dropna(subset=[\"ARIMA_Return_Forecast\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391ff986-812f-4189-9e5e-7e8168c19459",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.info())\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3499d5-5592-4398-8352-3b25b0d68ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 0) IMPORTS\n",
    "# ===============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# ARIMA\n",
    "from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ValueWarning)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported.\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 1) PREPARE DATA\n",
    "# ===============================================================\n",
    "data = data.sort_index()\n",
    "\n",
    "if \"Return\" not in data.columns:\n",
    "    raise ValueError(\"The dataset must contain a column named 'Return'.\")\n",
    "\n",
    "if \"Direction\" not in data.columns:\n",
    "    raise ValueError(\"The dataset must contain a column named 'Direction'.\")\n",
    "\n",
    "print(\"Dataset loaded & sorted.\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 2) CREATE ARIMA FEATURE\n",
    "# ===============================================================\n",
    "print(\"Fitting ARIMA(2,1,2)...\")\n",
    "\n",
    "arima_model = ARIMA(data[\"Return\"], order=(2, 1, 2))\n",
    "arima_fit = arima_model.fit()\n",
    "\n",
    "forecast = arima_fit.predict(start=1, end=len(data), dynamic=False)\n",
    "forecast.index = data.index\n",
    "data[\"ARIMA_Return_Forecast\"] = forecast.shift(1)\n",
    "\n",
    "data = data.dropna(subset=[\"ARIMA_Return_Forecast\"])\n",
    "print(\"ARIMA_Return_Forecast feature created.\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 3) CLEAN FULL DATASET (NaN, inf, extreme values)\n",
    "# ===============================================================\n",
    "print(\"Cleaning NaN and infinite values...\")\n",
    "\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "data.fillna(method=\"ffill\", inplace=True)\n",
    "data.fillna(method=\"bfill\", inplace=True)\n",
    "\n",
    "# Cap extreme values (finance best practice)\n",
    "data = data.clip(lower=-1e6, upper=1e6)\n",
    "\n",
    "print(\"Cleaning completed.\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 4) FEATURE SELECTION (ANOVA — Top 20)\n",
    "# ===============================================================\n",
    "print(\"Running SelectKBest (ANOVA)...\")\n",
    "\n",
    "# Prepare features\n",
    "features = data.drop(columns=[\"Direction\"])\n",
    "target = data[\"Direction\"]\n",
    "\n",
    "# Standardize features for ANOVA\n",
    "scaler_fs = StandardScaler()\n",
    "X_scaled = scaler_fs.fit_transform(features)\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=20)\n",
    "selector.fit(X_scaled, target)\n",
    "\n",
    "ranking_df = pd.DataFrame({\n",
    "    \"Feature\": features.columns,\n",
    "    \"ANOVA_F_score\": selector.scores_,\n",
    "    \"p_value\": selector.pvalues_,\n",
    "    \"Selected\": selector.get_support()\n",
    "}).sort_values(by=\"ANOVA_F_score\", ascending=False)\n",
    "\n",
    "print(\"Top 20 features:\")\n",
    "top20_features = ranking_df.head(20)[\"Feature\"].tolist()\n",
    "print(top20_features)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 5) BUILD ML DATASET\n",
    "# ===============================================================\n",
    "X = data[top20_features]\n",
    "y = data[\"Direction\"]\n",
    "\n",
    "# Final cleaning before feeding models\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split (time series: no shuffling)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "print(\"Train/Test dataset ready.\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 6) MODEL SET\n",
    "# ===============================================================\n",
    "models = [\n",
    "    (\"LogReg\", LogisticRegression(max_iter=500)),\n",
    "    (\"RandomForest\", RandomForestClassifier(n_estimators=300)),\n",
    "    (\"ExtraTrees\", ExtraTreesClassifier(n_estimators=300)),\n",
    "    (\"GradientBoost\", GradientBoostingClassifier()),\n",
    "    (\"AdaBoost\", AdaBoostClassifier()),\n",
    "    (\"SVM\", SVC(probability=True)),\n",
    "    (\"KNN\", KNeighborsClassifier(n_neighbors=7)),\n",
    "    (\"MLP\", MLPClassifier(max_iter=500))\n",
    "]\n",
    "\n",
    "print(\"Models initialized.\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 7) TRAIN & EVALUATE\n",
    "# ===============================================================\n",
    "results = []\n",
    "\n",
    "for name, model in models:\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    results.append((name, acc, f1, auc))\n",
    "\n",
    "    print(f\"{name} — ACC: {acc:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 8) RESULTS TABLE\n",
    "# ===============================================================\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"F1\", \"ROC-AUC\"])\n",
    "print(\"\\nFinal Results:\")\n",
    "display(results_df.sort_values(by=\"ROC-AUC\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e523d0-8a93-4345-a652-56f827853fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# ADVANCED MODELS – ENSEMBLE & META-LEARNING COMPARISON\n",
    "# ===============================================================\n",
    "print(\"\\n====================================================\")\n",
    "print(\"   ADVANCED ML MODELS – ENSEMBLE & STACKING\")\n",
    "print(\"====================================================\\n\")\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    BaggingClassifier,\n",
    "    VotingClassifier,\n",
    "    StackingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# PREPROCESSING PIPELINE (Imputation + Scaling)\n",
    "# ===============================================================\n",
    "def make_pipeline(model):\n",
    "    return Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1) BASE MODELS (for consistency)\n",
    "# ---------------------------------------------------------------\n",
    "base_svm = SVC(kernel='rbf', probability=True, random_state=0)\n",
    "base_dt = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 2) TUNED MODELS WITH GRID SEARCH (with Pipeline)\n",
    "# ===============================================================\n",
    "param_svm = {\n",
    "    \"model__C\": [0.1, 1, 10],\n",
    "    \"model__gamma\": [\"scale\", 0.01, 0.001]\n",
    "}\n",
    "\n",
    "tuned_svm = GridSearchCV(\n",
    "    make_pipeline(SVC(probability=True, random_state=0)),\n",
    "    param_svm,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    error_score=\"raise\"\n",
    ")\n",
    "tuned_svm.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "param_dt = {\n",
    "    \"model__max_depth\": [3, 5, 7, None],\n",
    "    \"model__min_samples_split\": [2, 5, 10]\n",
    "}\n",
    "\n",
    "tuned_dt = GridSearchCV(\n",
    "    make_pipeline(DecisionTreeClassifier(random_state=0)),\n",
    "    param_dt,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    error_score=\"raise\"\n",
    ")\n",
    "tuned_dt.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 3) BAGGING MODELS (corrected for scikit-learn ≥ 1.4)\n",
    "# ===============================================================\n",
    "\n",
    "bagging_svm = make_pipeline(\n",
    "    BaggingClassifier(\n",
    "        estimator=SVC(kernel='rbf', probability=True),\n",
    "        n_estimators=20,\n",
    "        max_samples=0.8,\n",
    "        random_state=0\n",
    "    )\n",
    ")\n",
    "\n",
    "bagging_dt = make_pipeline(\n",
    "    BaggingClassifier(\n",
    "        estimator=DecisionTreeClassifier(),\n",
    "        n_estimators=50,\n",
    "        max_samples=0.8,\n",
    "        random_state=0\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 4) VOTING CLASSIFIER (pipelined)\n",
    "# ===============================================================\n",
    "voting = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", VotingClassifier(\n",
    "        estimators=[\n",
    "            ('svm', SVC(kernel='rbf', probability=True)),\n",
    "            ('dt', DecisionTreeClassifier())\n",
    "        ],\n",
    "        voting='soft'\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 5) STACKING CLASSIFIER (with corrected Bagging)\n",
    "# ===============================================================\n",
    "stacking = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", StackingClassifier(\n",
    "        estimators=[\n",
    "            ('svm', SVC(kernel='rbf', probability=True)),\n",
    "            ('dt', DecisionTreeClassifier()),\n",
    "            ('bag_svm', BaggingClassifier(\n",
    "                estimator=SVC(kernel='rbf', probability=True),\n",
    "                n_estimators=10)),\n",
    "            ('bag_dt', BaggingClassifier(\n",
    "                estimator=DecisionTreeClassifier(),\n",
    "                n_estimators=10))\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(max_iter=500),\n",
    "        passthrough=True,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# TRAIN + EVALUATE ALL MODELS\n",
    "# ===============================================================\n",
    "models = {\n",
    "    \"Tuned SVM\": tuned_svm.best_estimator_,\n",
    "    \"Tuned Decision Tree\": tuned_dt.best_estimator_,\n",
    "    \"Bagging SVM\": bagging_svm,\n",
    "    \"Bagging Decision Tree\": bagging_dt,\n",
    "    \"Voting (SVM + DT)\": voting,\n",
    "    \"Stacking Meta-Model\": stacking\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n▶ Training {name} ...\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    results.append([name, acc, f1, auc])\n",
    "\n",
    "    print(f\"   Accuracy: {acc:.4f}\")\n",
    "    print(f\"   F1-score: {f1:.4f}\")\n",
    "    print(f\"   AUC: {auc:.4f}\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# SUMMARY TABLE\n",
    "# ===============================================================\n",
    "df_results = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"F1-score\", \"AUC\"])\n",
    "df_results.sort_values(by=\"F1-score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8285e2b7-9db8-4402-b361-f7ceabf3a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 0) IMPORTS\n",
    "# ===============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier, GradientBoostingClassifier,\n",
    "    RandomForestClassifier, ExtraTreesClassifier\n",
    ")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# ===============================================================\n",
    "# 1) SORT DATA & CHECK COLUMNS\n",
    "# ===============================================================\n",
    "# Make sure data is sorted in time\n",
    "data = data.sort_index()\n",
    "\n",
    "required_cols = [\"Return\", \"Direction\"]\n",
    "for c in required_cols:\n",
    "    if c not in data.columns:\n",
    "        raise ValueError(f\"Missing required column: {c}\")\n",
    "\n",
    "# ===============================================================\n",
    "# 2) BUILD ARIMA FORECAST FEATURE\n",
    "# ===============================================================\n",
    "# ARIMA on 'Return' – order (2,1,2) as we discussed\n",
    "arima_model = ARIMA(data[\"Return\"], order=(2, 1, 2))\n",
    "arima_fit = arima_model.fit()\n",
    "\n",
    "print(arima_fit.summary())  # optional\n",
    "\n",
    "# In-sample one-step-ahead predictions\n",
    "arima_pred = arima_fit.predict(start=1, end=len(data), dynamic=False)\n",
    "\n",
    "# Align index with data\n",
    "arima_pred.index = data.index\n",
    "\n",
    "# Shift by 1: forecast at t is based only on information up to t-1\n",
    "data[\"ARIMA_Return_Forecast\"] = arima_pred.shift(1)\n",
    "\n",
    "# You can also create a directional signal from ARIMA, optional:\n",
    "data[\"ARIMA_Direction\"] = (data[\"ARIMA_Return_Forecast\"] > 0).astype(int)\n",
    "\n",
    "# Because of the shift, first row is NaN → drop it (or more if needed)\n",
    "data_model = data.dropna(subset=[\"ARIMA_Return_Forecast\"]).copy()\n",
    "\n",
    "print(\"Data shape after adding ARIMA feature and dropping NaNs:\", data_model.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 3) DEFINE FEATURES & TARGET\n",
    "# ===============================================================\n",
    "# Drop raw price & return & target, keep everything else including ARIMA feature\n",
    "X = data_model.drop(columns=[\"Apple\", \"Return\", \"Direction\"])\n",
    "y = data_model[\"Direction\"]\n",
    "\n",
    "print(\"Final feature columns used:\")\n",
    "print(X.columns.tolist())\n",
    "\n",
    "# ===============================================================\n",
    "# 4) TRAIN / TEST SPLIT (TIME-SERIES FRIENDLY)\n",
    "# ===============================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \" Test shape:\", X_test.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 5) DEFINE CLASSIFICATION MODELS\n",
    "# ===============================================================\n",
    "models = [\n",
    "    ('LR',  LogisticRegression(max_iter=5000, random_state=RANDOM_STATE)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('CART', DecisionTreeClassifier(max_depth=6, random_state=RANDOM_STATE)),\n",
    "    ('SVC', SVC(kernel='rbf', probability=True, random_state=RANDOM_STATE)),\n",
    "    ('MLP', MLPClassifier(hidden_layer_sizes=(64, 32),\n",
    "                          max_iter=5000, random_state=RANDOM_STATE)),\n",
    "    # Boosting\n",
    "    ('ABR', AdaBoostClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('GBR', GradientBoostingClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    # Bagging\n",
    "    ('RFR', RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('ETR', ExtraTreesClassifier(n_estimators=300, random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "# ===============================================================\n",
    "# 6) EVALUATE MODELS WITH ARIMA FEATURE\n",
    "# ===============================================================\n",
    "results = []\n",
    "\n",
    "print(\"\\nRunning classification models with ARIMA feature...\\n\")\n",
    "\n",
    "for name, model in models:\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1  = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    results.append([name, acc, f1, auc])\n",
    "\n",
    "    print(f\"{name}: \"\n",
    "          f\"Accuracy={acc:.4f} | F1={f1:.4f} | ROC-AUC={auc:.4f}\")\n",
    "\n",
    "# ===============================================================\n",
    "# 7) RESULTS TABLE\n",
    "# ===============================================================\n",
    "results_df = pd.DataFrame(\n",
    "    results, columns=[\"Model\", \"Accuracy\", \"F1\", \"ROC_AUC\"]\n",
    ").sort_values(by=\"ROC_AUC\", ascending=False)\n",
    "\n",
    "print(\"\\n=========== Ranked Results (by ROC-AUC) ===========\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d80a88-efa5-49e6-bb82-95f43a6594b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 0) IMPORTS\n",
    "# ===============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# ML Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier, GradientBoostingClassifier,\n",
    "    RandomForestClassifier, ExtraTreesClassifier\n",
    ")\n",
    "\n",
    "# Deep Learning\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# ===============================================================\n",
    "# 1) SORT DATA\n",
    "# ===============================================================\n",
    "data = data.sort_index()\n",
    "\n",
    "# ===============================================================\n",
    "# 2) BUILD SUPERVISED SEQUENCE DATA FOR LSTM\n",
    "#    Here we use past 20 returns to predict return(t+1)\n",
    "# ===============================================================\n",
    "SEQ_LEN = 20\n",
    "\n",
    "returns = data[\"Return\"].values.reshape(-1, 1)\n",
    "\n",
    "X_seq = []\n",
    "y_seq = []\n",
    "\n",
    "for i in range(SEQ_LEN, len(returns)):\n",
    "    X_seq.append(returns[i-SEQ_LEN:i])\n",
    "    y_seq.append(returns[i])\n",
    "\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "print(\"LSTM input shape:\", X_seq.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 3) TRAIN/TEST SPLIT (no shuffle)\n",
    "# ===============================================================\n",
    "split = int(0.8 * len(X_seq))\n",
    "\n",
    "X_train_seq = X_seq[:split]\n",
    "X_test_seq  = X_seq[split:]\n",
    "\n",
    "y_train_seq = y_seq[:split]\n",
    "y_test_seq  = y_seq[split:]\n",
    "\n",
    "# ===============================================================\n",
    "# 4) DEFINE LSTM MODEL\n",
    "# ===============================================================\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(SEQ_LEN, 1)),\n",
    "    LSTM(32),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# ===============================================================\n",
    "# 5) TRAIN LSTM\n",
    "# ===============================================================\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_split=0.1,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ===============================================================\n",
    "# 6) FORECAST RETURNS WITH LSTM\n",
    "# ===============================================================\n",
    "lstm_forecast = model.predict(X_seq).flatten()\n",
    "\n",
    "# Shift by 1 to avoid lookahead bias\n",
    "lstm_forecast = pd.Series(lstm_forecast, index=data.index[SEQ_LEN:])\n",
    "lstm_forecast = lstm_forecast.shift(1)\n",
    "\n",
    "data[\"LSTM_Return_Forecast\"] = lstm_forecast\n",
    "\n",
    "# Optional: binary directional prediction from LSTM\n",
    "data[\"LSTM_Direction\"] = (data[\"LSTM_Return_Forecast\"] > 0).astype(int)\n",
    "\n",
    "# Drop the initial NaNs created by LSTM sequence length\n",
    "data_ml = data.dropna(subset=[\"LSTM_Return_Forecast\"])\n",
    "\n",
    "print(\"Data shape after adding LSTM feature:\", data_ml.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 7) PREPARE ML CLASSIFICATION DATA\n",
    "# ===============================================================\n",
    "X = data_ml.drop(columns=[\"Apple\", \"Return\", \"Direction\"])\n",
    "y = data_ml[\"Direction\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# ===============================================================\n",
    "# 8) DEFINE ML MODELS\n",
    "# ===============================================================\n",
    "models = [\n",
    "    ('LR',  LogisticRegression(max_iter=5000, random_state=RANDOM_STATE)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('CART', DecisionTreeClassifier(max_depth=6, random_state=RANDOM_STATE)),\n",
    "    ('SVC', SVC(kernel='rbf', probability=True, random_state=RANDOM_STATE)),\n",
    "    ('MLP', MLPClassifier(hidden_layer_sizes=(64, 32),\n",
    "                          max_iter=5000, random_state=RANDOM_STATE)),\n",
    "    ('ABR', AdaBoostClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('GBR', GradientBoostingClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('RFR', RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('ETR', ExtraTreesClassifier(n_estimators=300, random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "# ===============================================================\n",
    "# 9) EVALUATE THE ML CLASSIFIERS (WITH LSTM FEATURE)\n",
    "# ===============================================================\n",
    "results = []\n",
    "\n",
    "print(\"\\nRunning ML classifiers WITH LSTM feature...\\n\")\n",
    "\n",
    "for name, model_ml in models:\n",
    "    model_ml.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model_ml.predict(X_test)\n",
    "    y_proba = model_ml.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1  = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    results.append([name, acc, f1, auc])\n",
    "\n",
    "    print(f\"{name}: Accuracy={acc:.4f} | F1={f1:.4f} | ROC-AUC={auc:.4f}\")\n",
    "\n",
    "# ===============================================================\n",
    "# 10) RESULTS TABLE\n",
    "# ===============================================================\n",
    "results_df = pd.DataFrame(\n",
    "    results, columns=[\"Model\", \"Accuracy\", \"F1\", \"ROC_AUC\"]\n",
    ").sort_values(by=\"ROC_AUC\", ascending=False)\n",
    "\n",
    "print(\"\\n=========== Ranked Results (LSTM-enhanced ML) ===========\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cc9592-6229-4e53-881f-05e09d84251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 0) IMPORTS\n",
    "# ===============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier, GradientBoostingClassifier,\n",
    "    RandomForestClassifier, ExtraTreesClassifier\n",
    ")\n",
    "\n",
    "# Deep learning (LSTM)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# ===============================================================\n",
    "# 1) DATA PREP: SORT + BASIC CHECKS\n",
    "# ===============================================================\n",
    "# Assume 'data' already exists with 'Return' and 'Direction'\n",
    "data = data.sort_index()\n",
    "\n",
    "if \"Return\" not in data.columns or \"Direction\" not in data.columns:\n",
    "    raise ValueError(\"Data must contain 'Return' and 'Direction' columns.\")\n",
    "\n",
    "# ===============================================================\n",
    "# 2) ARIMA FORECAST: ARIMA_Return_Forecast\n",
    "# ===============================================================\n",
    "print(\"Fitting ARIMA(2,1,2) on returns...\")\n",
    "\n",
    "arima_model = ARIMA(data[\"Return\"], order=(2, 1, 2))\n",
    "arima_fit = arima_model.fit()\n",
    "\n",
    "arima_pred = arima_fit.predict(start=1, end=len(data), dynamic=False)\n",
    "arima_pred.index = data.index\n",
    "data[\"ARIMA_Return_Forecast\"] = arima_pred.shift(1)  # avoid look-ahead\n",
    "\n",
    "# Optional directional signal from ARIMA\n",
    "data[\"ARIMA_Direction\"] = (data[\"ARIMA_Return_Forecast\"] > 0).astype(int)\n",
    "\n",
    "# ===============================================================\n",
    "# 3) LSTM FORECAST: LSTM_Return_Forecast\n",
    "# ===============================================================\n",
    "print(\"Building LSTM sequences...\")\n",
    "\n",
    "SEQ_LEN = 20\n",
    "returns = data[\"Return\"].values.reshape(-1, 1)\n",
    "\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(SEQ_LEN, len(returns)):\n",
    "    X_seq.append(returns[i-SEQ_LEN:i])\n",
    "    y_seq.append(returns[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "print(\"LSTM input shape:\", X_seq.shape)\n",
    "\n",
    "# Split for LSTM (purely for forecasting, still time-ordered)\n",
    "split_idx = int(0.8 * len(X_seq))\n",
    "X_train_seq, X_test_seq = X_seq[:split_idx], X_seq[split_idx:]\n",
    "y_train_seq, y_test_seq = y_seq[:split_idx], y_seq[split_idx:]\n",
    "\n",
    "# Define LSTM\n",
    "model_lstm = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(SEQ_LEN, 1)),\n",
    "    LSTM(32),\n",
    "    Dense(1)\n",
    "])\n",
    "model_lstm.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "print(\"Training LSTM...\")\n",
    "history = model_lstm.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_split=0.1,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Generating LSTM forecasts...\")\n",
    "lstm_forecast = model_lstm.predict(X_seq).flatten()\n",
    "\n",
    "# Align with dates: first forecast corresponds to index[SEQ_LEN:]\n",
    "lstm_series = pd.Series(lstm_forecast, index=data.index[SEQ_LEN:])\n",
    "# Shift by 1 to avoid look-ahead\n",
    "lstm_series = lstm_series.shift(1)\n",
    "\n",
    "data[\"LSTM_Return_Forecast\"] = lstm_series\n",
    "data[\"LSTM_Direction\"] = (data[\"LSTM_Return_Forecast\"] > 0).astype(int)\n",
    "\n",
    "# ===============================================================\n",
    "# 4) DROP NaNs CREATED BY ARIMA/LSTM\n",
    "# ===============================================================\n",
    "data_ml = data.dropna(subset=[\"ARIMA_Return_Forecast\", \"LSTM_Return_Forecast\"]).copy()\n",
    "data_ml = data_ml.sort_index()\n",
    "\n",
    "print(\"Data shape after ARIMA+LSTM features:\", data_ml.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 5) FEATURES & TARGET FOR ML\n",
    "# ===============================================================\n",
    "X_full = data_ml.drop(columns=[\"Apple\", \"Return\", \"Direction\"])\n",
    "y_full = data_ml[\"Direction\"]\n",
    "\n",
    "# Time-safe train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "\n",
    "# Keep ARIMA/LSTM features separately for meta-layer\n",
    "arima_train = X_train[\"ARIMA_Return_Forecast\"].values.reshape(-1, 1)\n",
    "lstm_train  = X_train[\"LSTM_Return_Forecast\"].values.reshape(-1, 1)\n",
    "\n",
    "arima_test = X_test[\"ARIMA_Return_Forecast\"].values.reshape(-1, 1)\n",
    "lstm_test  = X_test[\"LSTM_Return_Forecast\"].values.reshape(-1, 1)\n",
    "\n",
    "# Base ML features without ARIMA/LSTM if you want to avoid duplicating them\n",
    "base_feature_cols = [c for c in X_train.columns\n",
    "                     if c not in [\"ARIMA_Return_Forecast\", \"LSTM_Return_Forecast\"]]\n",
    "X_train_base = X_train[base_feature_cols]\n",
    "X_test_base  = X_test[base_feature_cols]\n",
    "\n",
    "print(\"Base feature count (excluding ARIMA/LSTM):\", len(base_feature_cols))\n",
    "\n",
    "# ===============================================================\n",
    "# 6) DEFINE BASE CLASSIFICATION MODELS\n",
    "# ===============================================================\n",
    "base_models = [\n",
    "    ('LR',  LogisticRegression(max_iter=5000, random_state=RANDOM_STATE)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('CART', DecisionTreeClassifier(max_depth=6, random_state=RANDOM_STATE)),\n",
    "    ('SVC', SVC(kernel='rbf', probability=True, random_state=RANDOM_STATE)),\n",
    "    ('MLP', MLPClassifier(hidden_layer_sizes=(64, 32),\n",
    "                          max_iter=5000, random_state=RANDOM_STATE)),\n",
    "    ('ABR', AdaBoostClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('GBR', GradientBoostingClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('RFR', RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('ETR', ExtraTreesClassifier(n_estimators=300, random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "# ===============================================================\n",
    "# 7) STACKING – BUILD OUT-OF-FOLD META-FEATURES (TRAIN SET)\n",
    "# ===============================================================\n",
    "print(\"\\nBuilding out-of-fold meta-features (stacking)...\")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "n_train = X_train_base.shape[0]\n",
    "n_models = len(base_models)\n",
    "\n",
    "# Only base models here: shape (n_train, n_models)\n",
    "meta_train_base = np.full((n_train, n_models), np.nan)\n",
    "\n",
    "for m_idx, (name, model) in enumerate(base_models):\n",
    "    print(f\"  OOF for base model: {name}\")\n",
    "\n",
    "    oof_pred = np.full(n_train, np.nan)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train_base)):\n",
    "        X_tr, X_val = X_train_base.iloc[train_idx], X_train_base.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        proba_val = model.predict_proba(X_val)[:, 1]\n",
    "        oof_pred[val_idx] = proba_val\n",
    "\n",
    "    # Now fill missing OOF predictions (earlier samples never validated)\n",
    "    if np.isnan(oof_pred).any():\n",
    "        model.fit(X_train_base, y_train)\n",
    "        full_pred = model.predict_proba(X_train_base)[:, 1]\n",
    "        oof_pred = np.where(np.isnan(oof_pred), full_pred, oof_pred)\n",
    "\n",
    "    meta_train_base[:, m_idx] = oof_pred\n",
    "\n",
    "# Combine with ARIMA and LSTM features\n",
    "meta_features_train = np.concatenate(\n",
    "    [meta_train_base, arima_train, lstm_train], axis=1\n",
    ")\n",
    "\n",
    "print(\"Final meta_features_train shape:\", meta_features_train.shape)  # MUST be (N, 11)\n",
    "\n",
    "# ===============================================================\n",
    "# 8) TRAIN META MODEL\n",
    "# ===============================================================\n",
    "meta_clf = LogisticRegression(max_iter=5000, random_state=RANDOM_STATE)\n",
    "meta_clf.fit(meta_features_train, y_train)\n",
    "\n",
    "# ===============================================================\n",
    "# 9) BUILD META-FEATURES FOR TEST SET\n",
    "# ===============================================================\n",
    "print(\"\\nBuilding meta-features for TEST set...\")\n",
    "\n",
    "meta_test_base = np.zeros((X_test_base.shape[0], n_models))\n",
    "\n",
    "for m_idx, (name, model) in enumerate(base_models):\n",
    "    print(f\"  Train full + predict test for: {name}\")\n",
    "    model.fit(X_train_base, y_train)\n",
    "    meta_test_base[:, m_idx] = model.predict_proba(X_test_base)[:, 1]\n",
    "\n",
    "# Add ARIMA + LSTM\n",
    "meta_features_test = np.concatenate(\n",
    "    [meta_test_base, arima_test, lstm_test], axis=1\n",
    ")\n",
    "\n",
    "print(\"Final meta_features_test shape:\", meta_features_test.shape)  # MUST be (N_test, 11)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 10) EVALUATE META-MODEL\n",
    "# ===============================================================\n",
    "y_pred_meta = meta_clf.predict(meta_features_test)\n",
    "y_proba_meta = meta_clf.predict_proba(meta_features_test)[:, 1]\n",
    "\n",
    "acc_meta = accuracy_score(y_test, y_pred_meta)\n",
    "f1_meta  = f1_score(y_test, y_pred_meta)\n",
    "auc_meta = roc_auc_score(y_test, y_proba_meta)\n",
    "\n",
    "print(\"\\n=========== META-MODEL PERFORMANCE (ARIMA + LSTM + ML) ===========\")\n",
    "print(f\"Accuracy = {acc_meta:.4f}\")\n",
    "print(f\"F1       = {f1_meta:.4f}\")\n",
    "print(f\"ROC-AUC  = {auc_meta:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae2410b-3f6f-44f4-adb2-2965277945b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 0) IMPORTS\n",
    "# ===============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier, GradientBoostingClassifier,\n",
    "    RandomForestClassifier, ExtraTreesClassifier\n",
    ")\n",
    "\n",
    "# Deep learning (LSTM)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# ===============================================================\n",
    "# 1) PREP DATA + CREATE 8-DAY FORWARD TREND TARGET\n",
    "# ===============================================================\n",
    "data = data.sort_index()\n",
    "\n",
    "required_cols = [\"Apple\", \"Return\", \"Direction\"]\n",
    "for c in required_cols:\n",
    "    if c not in data.columns:\n",
    "        raise ValueError(f\"Missing column: {c}\")\n",
    "\n",
    "# Forward-looking 8-day average of daily Direction\n",
    "# Direction_Forward8(t) = mean(Direction(t+1..t+8))\n",
    "data[\"Direction_Forward8\"] = (\n",
    "    data[\"Direction\"]\n",
    "    .rolling(8)\n",
    "    .mean()\n",
    "    .shift(-8)\n",
    ")\n",
    "\n",
    "# Binary trend label:\n",
    "#  1 if > 60% of future 8 days are up\n",
    "#  0 if < 40% of future 8 days are up\n",
    "#  NaN in between (neutral, we drop them for now)\n",
    "data[\"Trend_8D\"] = np.where(\n",
    "    data[\"Direction_Forward8\"] >= 0.6, 1,\n",
    "    np.where(data[\"Direction_Forward8\"] <= 0.4, 0, np.nan)\n",
    ")\n",
    "\n",
    "# Drop rows where target is NaN (start and end)\n",
    "data = data.dropna(subset=[\"Trend_8D\"]).copy()\n",
    "data[\"Trend_8D\"] = data[\"Trend_8D\"].astype(int)\n",
    "\n",
    "print(\"Data shape after building Trend_8D target:\", data.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 2) ARIMA FORECAST FEATURE (ON RETURN)\n",
    "# ===============================================================\n",
    "print(\"Fitting ARIMA(2,1,2) on Return...\")\n",
    "\n",
    "arima_model = ARIMA(data[\"Return\"], order=(2, 1, 2))\n",
    "arima_fit = arima_model.fit()\n",
    "\n",
    "arima_pred = arima_fit.predict(start=1, end=len(data), dynamic=False)\n",
    "arima_pred.index = data.index\n",
    "\n",
    "# Shift one step to avoid look-ahead\n",
    "data[\"ARIMA_Return_Forecast\"] = arima_pred.shift(1)\n",
    "data[\"ARIMA_Direction\"] = (data[\"ARIMA_Return_Forecast\"] > 0).astype(int)\n",
    "\n",
    "# ===============================================================\n",
    "# 3) LSTM FORECAST FEATURE (ON RETURN)\n",
    "# ===============================================================\n",
    "print(\"Preparing LSTM sequences...\")\n",
    "\n",
    "SEQ_LEN = 20\n",
    "returns_arr = data[\"Return\"].values.reshape(-1, 1)\n",
    "\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(SEQ_LEN, len(returns_arr)):\n",
    "    X_seq.append(returns_arr[i-SEQ_LEN:i])\n",
    "    y_seq.append(returns_arr[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "print(\"LSTM input shape:\", X_seq.shape)\n",
    "\n",
    "split_idx = int(0.8 * len(X_seq))\n",
    "X_train_seq, X_test_seq = X_seq[:split_idx], X_seq[split_idx:]\n",
    "y_train_seq, y_test_seq = y_seq[:split_idx], y_seq[split_idx:]\n",
    "\n",
    "model_lstm = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(SEQ_LEN, 1)),\n",
    "    LSTM(32),\n",
    "    Dense(1)\n",
    "])\n",
    "model_lstm.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "print(\"Training LSTM...\")\n",
    "model_lstm.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_split=0.1,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Forecasting with LSTM...\")\n",
    "lstm_pred = model_lstm.predict(X_seq).flatten()\n",
    "lstm_series = pd.Series(lstm_pred, index=data.index[SEQ_LEN:])\n",
    "\n",
    "# Shift by 1 to avoid look-ahead\n",
    "data[\"LSTM_Return_Forecast\"] = lstm_series.shift(1)\n",
    "data[\"LSTM_Direction\"] = (data[\"LSTM_Return_Forecast\"] > 0).astype(int)\n",
    "\n",
    "# ===============================================================\n",
    "# 4) DROP NaNs (from ARIMA/LSTM shifts and sequence start)\n",
    "# ===============================================================\n",
    "data_ml = data.dropna(subset=[\"ARIMA_Return_Forecast\", \"LSTM_Return_Forecast\"]).copy()\n",
    "print(\"Data shape after ARIMA+LSTM dropna:\", data_ml.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 5) FEATURES & TARGET FOR STACKING (Option C)\n",
    "# ===============================================================\n",
    "# Use ALL available predictors except:\n",
    "#  - raw price/return\n",
    "#  - daily Direction\n",
    "#  - forward-label helpers\n",
    "X_full = data_ml.drop(columns=[\n",
    "    \"Apple\", \"Return\", \"Direction\", \"Direction_Forward8\"\n",
    "])\n",
    "y_full = data_ml[\"Trend_8D\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# Keep ARIMA/LSTM forecasts separate for meta-layer\n",
    "arima_train = X_train[\"ARIMA_Return_Forecast\"].values.reshape(-1, 1)\n",
    "lstm_train  = X_train[\"LSTM_Return_Forecast\"].values.reshape(-1, 1)\n",
    "arima_test  = X_test[\"ARIMA_Return_Forecast\"].values.reshape(-1, 1)\n",
    "lstm_test   = X_test[\"LSTM_Return_Forecast\"].values.reshape(-1, 1)\n",
    "\n",
    "# Base feature set excludes ARIMA/LSTM forecasts\n",
    "base_feature_cols = [\n",
    "    c for c in X_train.columns\n",
    "    if c not in [\"ARIMA_Return_Forecast\", \"LSTM_Return_Forecast\"]\n",
    "]\n",
    "X_train_base = X_train[base_feature_cols]\n",
    "X_test_base  = X_test[base_feature_cols]\n",
    "\n",
    "print(\"Base feature count (excluding ARIMA/LSTM):\", len(base_feature_cols))\n",
    "\n",
    "# ===============================================================\n",
    "# 6) DEFINE BASE CLASSIFICATION MODELS\n",
    "# ===============================================================\n",
    "base_models = [\n",
    "    ('LR',  LogisticRegression(max_iter=5000, random_state=RANDOM_STATE)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('CART', DecisionTreeClassifier(max_depth=6, random_state=RANDOM_STATE)),\n",
    "    ('SVC',  SVC(kernel='rbf', probability=True, random_state=RANDOM_STATE)),\n",
    "    ('MLP',  MLPClassifier(hidden_layer_sizes=(64, 32),\n",
    "                           max_iter=5000, random_state=RANDOM_STATE)),\n",
    "    ('ABR',  AdaBoostClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('GBR',  GradientBoostingClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('RFR',  RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('ETR',  ExtraTreesClassifier(n_estimators=300, random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "# ===============================================================\n",
    "# 7) STACKING – BUILD OUT-OF-FOLD META-FEATURES (TRAIN)\n",
    "# ===============================================================\n",
    "print(\"\\nBuilding out-of-fold meta-features for Trend_8D...\\n\")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "n_train = X_train_base.shape[0]\n",
    "n_models = len(base_models)\n",
    "\n",
    "meta_train_base = np.full((n_train, n_models), np.nan)\n",
    "\n",
    "for m_idx, (name, model) in enumerate(base_models):\n",
    "    print(f\"  {name} OOF predictions...\")\n",
    "    oof_pred = np.full(n_train, np.nan)\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(tscv.split(X_train_base)):\n",
    "        X_tr, X_val = X_train_base.iloc[tr_idx], X_train_base.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        oof_pred[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Fill NaNs (early samples not in any validation fold)\n",
    "    if np.isnan(oof_pred).any():\n",
    "        model.fit(X_train_base, y_train)\n",
    "        full_pred = model.predict_proba(X_train_base)[:, 1]\n",
    "        oof_pred = np.where(np.isnan(oof_pred), full_pred, oof_pred)\n",
    "\n",
    "    meta_train_base[:, m_idx] = oof_pred\n",
    "\n",
    "# Append ARIMA & LSTM forecasts to meta-features\n",
    "meta_features_train = np.concatenate(\n",
    "    [meta_train_base, arima_train, lstm_train],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"meta_features_train shape:\", meta_features_train.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 8) TRAIN META-CLASSIFIER (LOGISTIC REGRESSION)\n",
    "# ===============================================================\n",
    "meta_clf = LogisticRegression(max_iter=5000, random_state=RANDOM_STATE)\n",
    "meta_clf.fit(meta_features_train, y_train)\n",
    "\n",
    "# ===============================================================\n",
    "# 9) META-FEATURES FOR TEST\n",
    "# ===============================================================\n",
    "print(\"\\nBuilding meta-features for TEST...\\n\")\n",
    "\n",
    "meta_test_base = np.zeros((X_test_base.shape[0], n_models))\n",
    "\n",
    "for m_idx, (name, model) in enumerate(base_models):\n",
    "    print(f\"  {name} full-train → test prediction...\")\n",
    "    model.fit(X_train_base, y_train)\n",
    "    meta_test_base[:, m_idx] = model.predict_proba(X_test_base)[:, 1]\n",
    "\n",
    "meta_features_test = np.concatenate(\n",
    "    [meta_test_base, arima_test, lstm_test],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"meta_features_test shape:\", meta_features_test.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 10) EVALUATE META-MODEL ON Trend_8D (NO SMOOTHING)\n",
    "# ===============================================================\n",
    "y_proba_meta = meta_clf.predict_proba(meta_features_test)[:, 1]\n",
    "y_pred_meta  = (y_proba_meta >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n=========== META-MODEL PERFORMANCE on Trend_8D (raw) ===========\")\n",
    "print(f\"Accuracy = {accuracy_score(y_test, y_pred_meta):.4f}\")\n",
    "print(f\"F1       = {f1_score(y_test, y_pred_meta):.4f}\")\n",
    "print(f\"ROC-AUC  = {roc_auc_score(y_test, y_proba_meta):.4f}\")\n",
    "\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_meta))\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred_meta, digits=4))\n",
    "\n",
    "# ===============================================================\n",
    "# 11) SMOOTHED PREDICTION: USE LAST 8 DAYS MODEL PROBABILITIES\n",
    "#     forecast at T based on model probs from T-8..T-1\n",
    "# ===============================================================\n",
    "proba_series = pd.Series(y_proba_meta, index=X_test.index)\n",
    "\n",
    "# Rolling mean over past 8 predictions, aligned so that\n",
    "# proba_smooth(T) uses predictions from T-8..T-1\n",
    "proba_smooth = proba_series.rolling(window=8).mean().shift(1)\n",
    "\n",
    "y_pred_smooth = (proba_smooth >= 0.5).astype(int)\n",
    "\n",
    "# Align with y_test (drop NaNs from smoothing)\n",
    "valid_idx = proba_smooth.dropna().index\n",
    "y_test_smooth = y_test.loc[valid_idx]\n",
    "y_pred_smooth_valid = y_pred_smooth.loc[valid_idx]\n",
    "\n",
    "print(\"\\n=========== META-MODEL PERFORMANCE with 8-day SMOOTHED forecast ===========\")\n",
    "print(f\"Accuracy = {accuracy_score(y_test_smooth, y_pred_smooth_valid):.4f}\")\n",
    "print(f\"F1       = {f1_score(y_test_smooth, y_pred_smooth_valid):.4f}\")\n",
    "print(f\"ROC-AUC  = {roc_auc_score(y_test_smooth, proba_smooth.loc[valid_idx]):.4f}\")\n",
    "\n",
    "print(\"\\nConfusion matrix (smoothed):\")\n",
    "print(confusion_matrix(y_test_smooth, y_pred_smooth_valid))\n",
    "\n",
    "print(\"\\nClassification report (smoothed):\")\n",
    "print(classification_report(y_test_smooth, y_pred_smooth_valid, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11aadfc9-54a1-4220-a64f-b424cd2df916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3970 entries, 2010-07-29 to 2025-11-03\n",
      "Data columns (total 24 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   US10Y                  3970 non-null   float64\n",
      " 1   LQD                    3970 non-null   float64\n",
      " 2   HangSeng               3970 non-null   float64\n",
      " 3   IEF                    3970 non-null   float64\n",
      " 4   DowJones               3970 non-null   float64\n",
      " 5   BND                    3970 non-null   float64\n",
      " 6   TLT                    3970 non-null   float64\n",
      " 7   Nikkei225              3970 non-null   float64\n",
      " 8   MSCIWorld              3970 non-null   float64\n",
      " 9   MA20                   3970 non-null   float64\n",
      " 10  Imports_GDP_Pct        3970 non-null   float64\n",
      " 11  US2Y                   3970 non-null   float64\n",
      " 12  Momentum               3970 non-null   float64\n",
      " 13  S&P500                 3970 non-null   float64\n",
      " 14  Recession_Probability  3970 non-null   float64\n",
      " 15  Exports_GDP_Pct        3970 non-null   float64\n",
      " 16  DAX                    3970 non-null   float64\n",
      " 17  Inflation_Annual_Pct   3970 non-null   float64\n",
      " 18  Fed_Funds_Rate         3970 non-null   float64\n",
      " 19  CAC40                  3970 non-null   float64\n",
      " 20  Apple                  3970 non-null   float64\n",
      " 21  Return                 3970 non-null   float64\n",
      " 22  Direction              3970 non-null   int64  \n",
      " 23  target_index           3970 non-null   float64\n",
      "dtypes: float64(23), int64(1)\n",
      "memory usage: 775.4 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6393e7-2cb7-47d1-9417-ba66aadf96cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after Trend_8D creation: (2122, 26)\n",
      "Shape after cleaning: (2049, 22)\n",
      "Base feature count: 22\n",
      "\n",
      "Building out-of-fold meta-features...\n",
      "\n",
      "  LR OOF predictions...\n",
      "  KNN OOF predictions...\n",
      "  CART OOF predictions...\n",
      "  SVC OOF predictions...\n",
      "  MLP OOF predictions...\n",
      "  ABR OOF predictions...\n",
      "  GBR OOF predictions...\n",
      "  RFR OOF predictions...\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# 0) IMPORTS\n",
    "# ===============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier, GradientBoostingClassifier,\n",
    "    RandomForestClassifier, ExtraTreesClassifier\n",
    ")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# ===============================================================\n",
    "# 1) PREP DATA + CREATE 8-DAY FORWARD TREND TARGET\n",
    "# ===============================================================\n",
    "data = data.sort_index()\n",
    "\n",
    "required_cols = [\"Apple\", \"Return\", \"Direction\"]\n",
    "for c in required_cols:\n",
    "    if c not in data.columns:\n",
    "        raise ValueError(f\"Missing column: {c}\")\n",
    "\n",
    "# Forward 8-day trend\n",
    "data[\"Direction_Forward8\"] = (\n",
    "    data[\"Direction\"]\n",
    "    .rolling(8)\n",
    "    .mean()\n",
    "    .shift(-8)\n",
    ")\n",
    "\n",
    "# Binary label\n",
    "data[\"Trend_8D\"] = np.where(\n",
    "    data[\"Direction_Forward8\"] >= 0.6, 1,\n",
    "    np.where(data[\"Direction_Forward8\"] <= 0.4, 0, np.nan)\n",
    ")\n",
    "\n",
    "# Keep only valid rows\n",
    "data = data.dropna(subset=[\"Trend_8D\"]).copy()\n",
    "data[\"Trend_8D\"] = data[\"Trend_8D\"].astype(int)\n",
    "\n",
    "print(\"Data shape after Trend_8D creation:\", data.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 2) FEATURE SELECTION (NO ARIMA, NO LSTM)\n",
    "# ===============================================================\n",
    "X_full = data.drop(columns=[\"Apple\", \"Return\", \"Direction\", \"Direction_Forward8\"])\n",
    "y_full = data[\"Trend_8D\"]\n",
    "\n",
    "# -------- CLEANING STEP --------\n",
    "# Remove inf / -inf\n",
    "X_full = X_full.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Remove all NaN rows\n",
    "X_full = X_full.dropna(how=\"any\")\n",
    "y_full = y_full.loc[X_full.index]\n",
    "\n",
    "print(\"Shape after cleaning:\", X_full.shape)\n",
    "# --------------------------------\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "base_feature_cols = X_train.columns.tolist()\n",
    "print(\"Base feature count:\", len(base_feature_cols))\n",
    "\n",
    "# ===============================================================\n",
    "# 3) DEFINE BASE CLASSIFICATION MODELS\n",
    "# ===============================================================\n",
    "base_models = [\n",
    "    ('LR',  LogisticRegression(max_iter=5000, random_state=RANDOM_STATE)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('CART', DecisionTreeClassifier(max_depth=6, random_state=RANDOM_STATE)),\n",
    "    ('SVC',  SVC(kernel='rbf', probability=True, random_state=RANDOM_STATE)),\n",
    "    ('MLP',  MLPClassifier(hidden_layer_sizes=(64, 32),\n",
    "                           max_iter=5000, random_state=RANDOM_STATE)),\n",
    "    ('ABR',  AdaBoostClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('GBR',  GradientBoostingClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('RFR',  RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('ETR',  ExtraTreesClassifier(n_estimators=300, random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "# ===============================================================\n",
    "# 4) STACKING – BUILD OUT-OF-FOLD META-FEATURES (TRAIN)\n",
    "# ===============================================================\n",
    "print(\"\\nBuilding out-of-fold meta-features...\\n\")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "n_train = X_train.shape[0]\n",
    "n_models = len(base_models)\n",
    "\n",
    "meta_train = np.full((n_train, n_models), np.nan)\n",
    "\n",
    "for m_idx, (name, model) in enumerate(base_models):\n",
    "    print(f\"  {name} OOF predictions...\")\n",
    "    oof_pred = np.full(n_train, np.nan)\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(tscv.split(X_train)):\n",
    "        X_tr, X_val = X_train.iloc[tr_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        oof_pred[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Fill NaNs for early rows not included in validation sets\n",
    "    if np.isnan(oof_pred).any():\n",
    "        model.fit(X_train, y_train)\n",
    "        full_pred = model.predict_proba(X_train)[:, 1]\n",
    "        oof_pred = np.where(np.isnan(oof_pred), full_pred, oof_pred)\n",
    "\n",
    "    meta_train[:, m_idx] = oof_pred\n",
    "\n",
    "print(\"meta_train shape:\", meta_train.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 5) TRAIN META-CLASSIFIER\n",
    "# ===============================================================\n",
    "meta_clf = LogisticRegression(max_iter=5000, random_state=RANDOM_STATE)\n",
    "meta_clf.fit(meta_train, y_train)\n",
    "\n",
    "# ===============================================================\n",
    "# 6) META-FEATURES FOR TEST\n",
    "# ===============================================================\n",
    "print(\"\\nBuilding meta-features for TEST...\\n\")\n",
    "\n",
    "meta_test = np.zeros((X_test.shape[0], n_models))\n",
    "\n",
    "for m_idx, (name, model) in enumerate(base_models):\n",
    "    print(f\"  {name} full-train → test prediction...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    meta_test[:, m_idx] = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"meta_test shape:\", meta_test.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 7) EVALUATE META-MODEL (RAW)\n",
    "# ===============================================================\n",
    "y_proba_meta = meta_clf.predict_proba(meta_test)[:, 1]\n",
    "y_pred_meta  = (y_proba_meta >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n=========== META-MODEL PERFORMANCE (RAW) ===========\")\n",
    "print(f\"Accuracy = {accuracy_score(y_test, y_pred_meta):.4f}\")\n",
    "print(f\"F1       = {f1_score(y_test, y_pred_meta):.4f}\")\n",
    "print(f\"ROC-AUC  = {roc_auc_score(y_test, y_proba_meta):.4f}\")\n",
    "\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_meta))\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred_meta, digits=4))\n",
    "\n",
    "# ===============================================================\n",
    "# 8) 8-DAY SMOOTHED PREDICTIONS\n",
    "# ===============================================================\n",
    "proba_series = pd.Series(y_proba_meta, index=X_test.index)\n",
    "proba_smooth = proba_series.rolling(window=8).mean().shift(1)\n",
    "\n",
    "# Keep only valid values\n",
    "valid_idx = proba_smooth.dropna().index\n",
    "y_test_valid = y_test.loc[valid_idx]\n",
    "y_pred_smooth = (proba_smooth.loc[valid_idx] >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n=========== META-MODEL PERFORMANCE (8-DAY SMOOTHED) ===========\")\n",
    "print(f\"Accuracy = {accuracy_score(y_test_valid, y_pred_smooth):.4f}\")\n",
    "print(f\"F1       = {f1_score(y_test_valid, y_pred_smooth):.4f}\")\n",
    "print(f\"ROC-AUC  = {roc_auc_score(y_test_valid, proba_smooth.loc[valid_idx]):.4f}\")\n",
    "\n",
    "print(\"\\nConfusion matrix (smoothed):\")\n",
    "print(confusion_matrix(y_test_valid, y_pred_smooth))\n",
    "\n",
    "print(\"\\nClassification report (smoothed):\")\n",
    "print(classification_report(y_test_valid, y_pred_smooth, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6305aa73-99ba-4538-959b-2b09de1e3f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
