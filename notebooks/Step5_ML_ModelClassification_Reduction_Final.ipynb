{
 "cells": [
  {
   "cell_type": "raw",
   "id": "db24864f-ff4f-42c9-bafd-cb780aa055da",
   "metadata": {},
   "source": [
    "# ===============================================================\n",
    "# MACHINE LEARNING PROJECT - 2025 GUIDELINES COMPLIANT\n",
    "# Title: Stock Price Direction Forecasting using Macroeconomic Indicators\n",
    "# Author:\n",
    "# ===============================================================\n",
    "\n",
    "# Predicting Apple Stock Direction — Full ML Pipeline\n",
    "# This notebook follows the full structured process:\n",
    "\n",
    "1) Import libraries & dataset\n",
    "2) Create binary target (Apple_Direction)\n",
    "3) Data exploration & preprocessing\n",
    "4) Train/test split\n",
    "5) Train models\n",
    "6) Hyperparameter tuning\n",
    "7) Evaluate metrics & compare models\n",
    "8) PCA and overfitting checks\n",
    "9) Final comparison & conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f0694cd-7df9-4cab-950a-a305dc9b4b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 1) IMPORT LIBRARIES\n",
    "# ===============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report, RocCurveDisplay\n",
    "import shap\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e939bfd-5e2f-4da7-b5ca-4fac503faaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dax_a\\ml_outputs\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# 1) LOAD DATASETS\n",
    "# ===============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "OUT = Path.home() / \"ml_outputs\"\n",
    "print(OUT)\n",
    "\n",
    "data = pd.read_csv(OUT / 'Cleaned_Features_for_ML.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "850bbc61-2f57-45a9-85f1-f710d7c297cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset shape: (3970, 24)\n",
      "Columns kept: ['US10Y', 'LQD', 'HangSeng', 'IEF', 'DowJones', 'BND', 'TLT', 'Nikkei225', 'MSCIWorld', 'MA20', 'Imports_GDP_Pct', 'US2Y', 'Momentum', 'S&P500', 'Recession_Probability', 'Exports_GDP_Pct', 'DAX', 'Inflation_Annual_Pct', 'Fed_Funds_Rate', 'CAC40', 'Apple', 'Return', 'Direction', 'target_index']\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# STEP — REDUCE DATASET TO TOP 20 FEATURES + REQUIRED COLUMNS\n",
    "# ===============================================================\n",
    "\n",
    "# Top 20 selected features\n",
    "top20_features = [\n",
    "    \"US10Y\", \"LQD\", \"HangSeng\", \"IEF\", \"DowJones\", \"BND\", \"TLT\",\n",
    "    \"Nikkei225\", \"MSCIWorld\", \"MA20\", \"Imports_GDP_Pct\", \"US2Y\",\n",
    "    \"Momentum\", \"S&P500\", \"Recession_Probability\", \"Exports_GDP_Pct\",\n",
    "    \"DAX\", \"Inflation_Annual_Pct\", \"Fed_Funds_Rate\", \"CAC40\"\n",
    "]\n",
    "\n",
    "\n",
    "# Columns that MUST remain in the dataset\n",
    "mandatory_columns = [\"Apple\", \"Return\", \"Direction\", \"target_index\"]\n",
    "\n",
    "# Make sure they exist\n",
    "for col in mandatory_columns:\n",
    "    if col not in data.columns:\n",
    "        print(f\"Warning: column '{col}' was not found in data.\")\n",
    "\n",
    "# Build the new reduced dataframe\n",
    "keep_columns = top20_features + mandatory_columns\n",
    "data = data[keep_columns].copy()\n",
    "\n",
    "print(\"New dataset shape:\", data.shape)\n",
    "print(\"Columns kept:\", data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c954304-41a2-4855-9624-61165e5ddcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3970 entries, 2010-07-29 to 2025-11-03\n",
      "Data columns (total 24 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Recession_Probability    3970 non-null   float64\n",
      " 1   HangSeng                 3970 non-null   float64\n",
      " 2   LQD                      3970 non-null   float64\n",
      " 3   Unemployment_Rate        3970 non-null   float64\n",
      " 4   OECD_Unemp_rate_pct_USA  3970 non-null   float64\n",
      " 5   BND                      3970 non-null   float64\n",
      " 6   Imports_GDP_Pct          3970 non-null   float64\n",
      " 7   US10Y                    3970 non-null   float64\n",
      " 8   IEF                      3970 non-null   float64\n",
      " 9   Nikkei225                3970 non-null   float64\n",
      " 10  Fed_Funds_Rate           3970 non-null   float64\n",
      " 11  Inflation_Annual_Pct     3970 non-null   float64\n",
      " 12  Exports_GDP_Pct          3970 non-null   float64\n",
      " 13  TLT                      3970 non-null   float64\n",
      " 14  MA50                     3970 non-null   float64\n",
      " 15  Unemployment_Total_Pct   3970 non-null   float64\n",
      " 16  Yield_Curve_Spread       3970 non-null   float64\n",
      " 17  M2_Money_Supply          3970 non-null   float64\n",
      " 18  CAC40                    3970 non-null   float64\n",
      " 19  GDP_Current_USD          3970 non-null   float64\n",
      " 20  Apple                    3970 non-null   float64\n",
      " 21  Return                   3970 non-null   float64\n",
      " 22  Direction                3970 non-null   int64  \n",
      " 23  target_index             3970 non-null   float64\n",
      "dtypes: float64(23), int64(1)\n",
      "memory usage: 775.4 KB\n",
      "None\n",
      "            Recession_Probability  HangSeng       LQD  Unemployment_Rate  \\\n",
      "Date                                                                       \n",
      "2010-07-29                   0.06  0.000125  0.000364                9.4   \n",
      "2010-07-30                   0.06 -0.003035  0.004645                9.4   \n",
      "2010-08-02                   0.04  0.018211 -0.002120                9.5   \n",
      "2010-08-03                   0.04  0.002096  0.002463                9.5   \n",
      "2010-08-04                   0.04  0.004298 -0.003094                9.5   \n",
      "2010-08-05                   0.04  0.000085  0.001826                9.5   \n",
      "2010-08-06                   0.04  0.005897  0.005740                9.5   \n",
      "2010-08-09                   0.04  0.005664  0.003261                9.5   \n",
      "2010-08-10                   0.04 -0.015044 -0.001264                9.5   \n",
      "2010-08-11                   0.04 -0.008339 -0.000090                9.5   \n",
      "\n",
      "            OECD_Unemp_rate_pct_USA       BND  Imports_GDP_Pct     US10Y  \\\n",
      "Date                                                                       \n",
      "2010-07-29                      9.4  0.001102        15.878528 -0.000666   \n",
      "2010-07-30                      9.4  0.001468        15.878528 -0.030677   \n",
      "2010-08-02                      9.4 -0.000220        15.878528  0.019264   \n",
      "2010-08-03                      9.4  0.002696        15.878528 -0.016537   \n",
      "2010-08-04                      9.4 -0.001955        15.878528  0.013040   \n",
      "2010-08-05                      9.4  0.001592        15.878528 -0.012534   \n",
      "2010-08-06                      9.4  0.002934        15.878528 -0.031218   \n",
      "2010-08-09                      9.4 -0.001219        15.878528 -0.000708   \n",
      "2010-08-10                      9.4  0.002075        15.878528 -0.014529   \n",
      "2010-08-11                      9.4  0.000731        15.878528 -0.034520   \n",
      "\n",
      "                 IEF  Nikkei225  ...      MA50  Unemployment_Total_Pct  \\\n",
      "Date                             ...                                     \n",
      "2010-07-29  0.002094  -0.005870  ...  0.001790                   9.633   \n",
      "2010-07-30  0.005850  -0.016370  ...  0.001340                   9.633   \n",
      "2010-08-02 -0.003092   0.003461  ...  0.001331                   9.633   \n",
      "2010-08-03  0.004700   0.012925  ...  0.001462                   9.633   \n",
      "2010-08-04 -0.004159  -0.021113  ...  0.001633                   9.633   \n",
      "2010-08-05  0.004802   0.017344  ...  0.000779                   9.633   \n",
      "2010-08-06  0.006546  -0.001222  ...  0.000377                   9.633   \n",
      "2010-08-09 -0.000206  -0.007221  ...  0.000197                   9.633   \n",
      "2010-08-10  0.005058  -0.002240  ...  0.000018                   9.633   \n",
      "2010-08-11  0.005650  -0.027034  ... -0.000932                   9.633   \n",
      "\n",
      "            Yield_Curve_Spread  M2_Money_Supply     CAC40  GDP_Current_USD  \\\n",
      "Date                                                                         \n",
      "2010-07-29                2.88           8639.5 -0.005027     1.504897e+13   \n",
      "2010-07-30                2.79           8639.5 -0.002401     1.504897e+13   \n",
      "2010-08-02                2.83           8687.9  0.029889     1.504897e+13   \n",
      "2010-08-03                2.78           8687.9 -0.001205     1.504897e+13   \n",
      "2010-08-04                2.82           8687.9  0.003525     1.504897e+13   \n",
      "2010-08-05                2.79           8687.9  0.000923     1.504897e+13   \n",
      "2010-08-06                2.71           8687.9 -0.012789     1.504897e+13   \n",
      "2010-08-09                2.71           8687.9  0.016501     1.504897e+13   \n",
      "2010-08-10                2.64           8687.9 -0.012387     1.504897e+13   \n",
      "2010-08-11                2.57           8687.9 -0.027419     1.504897e+13   \n",
      "\n",
      "               Apple    Return  Direction  target_index  \n",
      "Date                                                     \n",
      "2010-07-29 -0.010922 -0.010922          0     -0.010922  \n",
      "2010-07-30 -0.003332 -0.003332          1     -0.003332  \n",
      "2010-08-02  0.017881  0.017881          1      0.017881  \n",
      "2010-08-03  0.000306  0.000306          1      0.000306  \n",
      "2010-08-04  0.004009  0.004009          0      0.004009  \n",
      "2010-08-05 -0.004867 -0.004867          0     -0.004867  \n",
      "2010-08-06 -0.006152 -0.006152          1     -0.006152  \n",
      "2010-08-09  0.006383  0.006383          0      0.006383  \n",
      "2010-08-10 -0.008940 -0.008940          0     -0.008940  \n",
      "2010-08-11 -0.035542 -0.035542          1     -0.035542  \n",
      "\n",
      "[10 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data.info())\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3289a7c5-76e5-4d1f-9496-37ef1194b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# SUPPRESS ARIMA WARNINGS SAFELY\n",
    "# ===============================================================\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ValueWarning)\n",
    "\n",
    "# ===============================================================\n",
    "# ARIMA MODEL\n",
    "# ===============================================================\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "data = data.sort_index()\n",
    "\n",
    "model = ARIMA(data[\"Return\"], order=(2,1,2))\n",
    "model_fit = model.fit()\n",
    "\n",
    "forecast = model_fit.predict(start=1, end=len(data), dynamic=False)\n",
    "forecast.index = data.index\n",
    "\n",
    "data[\"ARIMA_Return_Forecast\"] = forecast.shift(1)\n",
    "data = data.dropna(subset=[\"ARIMA_Return_Forecast\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "391ff986-812f-4189-9e5e-7e8168c19459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3968 entries, 2010-08-02 to 2025-11-03\n",
      "Data columns (total 25 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   US10Y                  3968 non-null   float64\n",
      " 1   LQD                    3968 non-null   float64\n",
      " 2   HangSeng               3968 non-null   float64\n",
      " 3   IEF                    3968 non-null   float64\n",
      " 4   DowJones               3968 non-null   float64\n",
      " 5   BND                    3968 non-null   float64\n",
      " 6   TLT                    3968 non-null   float64\n",
      " 7   Nikkei225              3968 non-null   float64\n",
      " 8   MSCIWorld              3968 non-null   float64\n",
      " 9   MA20                   3968 non-null   float64\n",
      " 10  Imports_GDP_Pct        3968 non-null   float64\n",
      " 11  US2Y                   3968 non-null   float64\n",
      " 12  Momentum               3968 non-null   float64\n",
      " 13  S&P500                 3968 non-null   float64\n",
      " 14  Recession_Probability  3968 non-null   float64\n",
      " 15  Exports_GDP_Pct        3968 non-null   float64\n",
      " 16  DAX                    3968 non-null   float64\n",
      " 17  Inflation_Annual_Pct   3968 non-null   float64\n",
      " 18  Fed_Funds_Rate         3968 non-null   float64\n",
      " 19  CAC40                  3968 non-null   float64\n",
      " 20  Apple                  3968 non-null   float64\n",
      " 21  Return                 3968 non-null   float64\n",
      " 22  Direction              3968 non-null   int64  \n",
      " 23  target_index           3968 non-null   float64\n",
      " 24  ARIMA_Return_Forecast  3968 non-null   float64\n",
      "dtypes: float64(24), int64(1)\n",
      "memory usage: 806.0 KB\n",
      "None\n",
      "               US10Y       LQD  HangSeng       IEF  DowJones       BND  \\\n",
      "Date                                                                     \n",
      "2010-08-02  0.019264 -0.002120  0.018211 -0.003092  0.019916 -0.000220   \n",
      "2010-08-03 -0.016537  0.002463  0.002096  0.004700 -0.003560  0.002696   \n",
      "2010-08-04  0.013040 -0.003094  0.004298 -0.004159  0.004141 -0.001955   \n",
      "2010-08-05 -0.012534  0.001826  0.000085  0.004802 -0.000510  0.001592   \n",
      "2010-08-06 -0.031218  0.005740  0.005897  0.006546 -0.002007  0.002934   \n",
      "2010-08-09 -0.000708  0.003261  0.005664 -0.000206  0.004242 -0.001219   \n",
      "2010-08-10 -0.014529 -0.001264 -0.015044  0.005058 -0.005094  0.002075   \n",
      "2010-08-11 -0.034520 -0.000090 -0.008339  0.005650 -0.024936  0.000731   \n",
      "2010-08-12  0.018622 -0.003527 -0.008867 -0.002553 -0.005673 -0.001826   \n",
      "2010-08-13 -0.017185  0.003901 -0.001618  0.004301 -0.001628  0.002805   \n",
      "\n",
      "                 TLT  Nikkei225  MSCIWorld      MA20  ...  Exports_GDP_Pct  \\\n",
      "Date                                                  ...                    \n",
      "2010-08-02 -0.014038   0.003461        0.0  0.002705  ...        12.341355   \n",
      "2010-08-03  0.005772   0.012925        0.0  0.002720  ...        12.341355   \n",
      "2010-08-04 -0.007653  -0.021113        0.0  0.000901  ...        12.341355   \n",
      "2010-08-05  0.004668   0.017344        0.0  0.000770  ...        12.341355   \n",
      "2010-08-06  0.010906  -0.001222        0.0  0.000166  ...        12.341355   \n",
      "2010-08-09 -0.003696  -0.007221        0.0  0.000934  ...        12.341355   \n",
      "2010-08-10  0.002105  -0.002240        0.0  0.001554  ...        12.341355   \n",
      "2010-08-11  0.013409  -0.027034        0.0 -0.000408  ...        12.341355   \n",
      "2010-08-12 -0.002469  -0.008637        0.0  0.000165  ...        12.341355   \n",
      "2010-08-13  0.012471   0.004436        0.0 -0.000061  ...        12.341355   \n",
      "\n",
      "                 DAX  Inflation_Annual_Pct  Fed_Funds_Rate     CAC40  \\\n",
      "Date                                                                   \n",
      "2010-08-02  0.023448              1.640043            0.19  0.029889   \n",
      "2010-08-03  0.002508              1.640043            0.19 -0.001205   \n",
      "2010-08-04  0.003713              1.640043            0.19  0.003525   \n",
      "2010-08-05  0.000355              1.640043            0.19  0.000923   \n",
      "2010-08-06 -0.011676              1.640043            0.19 -0.012789   \n",
      "2010-08-09  0.014693              1.640043            0.19  0.016501   \n",
      "2010-08-10 -0.010289              1.640043            0.19 -0.012387   \n",
      "2010-08-11 -0.021027              1.640043            0.19 -0.027419   \n",
      "2010-08-12 -0.003071              1.640043            0.19 -0.001990   \n",
      "2010-08-13 -0.004036              1.640043            0.19 -0.002806   \n",
      "\n",
      "               Apple    Return  Direction  target_index  ARIMA_Return_Forecast  \n",
      "Date                                                                            \n",
      "2010-08-02  0.017881  0.017881          1      0.017881              -0.003332  \n",
      "2010-08-03  0.000306  0.000306          1      0.000306               0.006990  \n",
      "2010-08-04  0.004009  0.004009          0      0.004009               0.005083  \n",
      "2010-08-05 -0.004867 -0.004867          0     -0.004867               0.004999  \n",
      "2010-08-06 -0.006152 -0.006152          1     -0.006152               0.002972  \n",
      "2010-08-09  0.006383  0.006383          0      0.006383               0.001834  \n",
      "2010-08-10 -0.008940 -0.008940          0     -0.008940               0.001743  \n",
      "2010-08-11 -0.035542 -0.035542          1     -0.035542               0.001136  \n",
      "2010-08-12  0.006395  0.006395          0      0.006395              -0.002176  \n",
      "2010-08-13 -0.010683 -0.010683          0     -0.010683              -0.002462  \n",
      "\n",
      "[10 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data.info())\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d3499d5-5592-4398-8352-3b25b0d68ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported.\n",
      "Dataset loaded & sorted.\n",
      "Fitting ARIMA(2,1,2)...\n",
      "ARIMA_Return_Forecast feature created.\n",
      "Cleaning NaN and infinite values...\n",
      "Cleaning completed.\n",
      "Running SelectKBest (ANOVA)...\n",
      "Top 20 features:\n",
      "['US10Y', 'LQD', 'HangSeng', 'IEF', 'DowJones', 'BND', 'TLT', 'Nikkei225', 'MSCIWorld', 'MA20', 'Imports_GDP_Pct', 'ARIMA_Return_Forecast', 'US2Y', 'Momentum', 'S&P500', 'Recession_Probability', 'Exports_GDP_Pct', 'DAX', 'Inflation_Annual_Pct', 'CAC40']\n",
      "Train/Test dataset ready.\n",
      "Models initialized.\n",
      "\n",
      "Training LogReg...\n",
      "LogReg — ACC: 0.5202, F1: 0.5809, AUC: 0.5143\n",
      "\n",
      "Training RandomForest...\n",
      "RandomForest — ACC: 0.5050, F1: 0.5549, AUC: 0.5031\n",
      "\n",
      "Training ExtraTrees...\n",
      "ExtraTrees — ACC: 0.5038, F1: 0.5651, AUC: 0.4934\n",
      "\n",
      "Training GradientBoost...\n",
      "GradientBoost — ACC: 0.5076, F1: 0.5295, AUC: 0.5016\n",
      "\n",
      "Training AdaBoost...\n",
      "AdaBoost — ACC: 0.5416, F1: 0.5583, AUC: 0.5297\n",
      "\n",
      "Training SVM...\n",
      "SVM — ACC: 0.5151, F1: 0.6214, AUC: 0.4625\n",
      "\n",
      "Training KNN...\n",
      "KNN — ACC: 0.4748, F1: 0.5341, AUC: 0.4692\n",
      "\n",
      "Training MLP...\n",
      "MLP — ACC: 0.4912, F1: 0.5618, AUC: 0.4839\n",
      "\n",
      "Final Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.541562</td>\n",
       "      <td>0.558252</td>\n",
       "      <td>0.529709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.520151</td>\n",
       "      <td>0.580858</td>\n",
       "      <td>0.514328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.505038</td>\n",
       "      <td>0.554926</td>\n",
       "      <td>0.503085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoost</td>\n",
       "      <td>0.507557</td>\n",
       "      <td>0.529483</td>\n",
       "      <td>0.501638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>0.503778</td>\n",
       "      <td>0.565121</td>\n",
       "      <td>0.493448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.491184</td>\n",
       "      <td>0.561822</td>\n",
       "      <td>0.483878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.474811</td>\n",
       "      <td>0.534078</td>\n",
       "      <td>0.469175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.515113</td>\n",
       "      <td>0.621436</td>\n",
       "      <td>0.462480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Accuracy        F1   ROC-AUC\n",
       "4       AdaBoost  0.541562  0.558252  0.529709\n",
       "0         LogReg  0.520151  0.580858  0.514328\n",
       "1   RandomForest  0.505038  0.554926  0.503085\n",
       "3  GradientBoost  0.507557  0.529483  0.501638\n",
       "2     ExtraTrees  0.503778  0.565121  0.493448\n",
       "7            MLP  0.491184  0.561822  0.483878\n",
       "6            KNN  0.474811  0.534078  0.469175\n",
       "5            SVM  0.515113  0.621436  0.462480"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# 0) IMPORTS\n",
    "# ===============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# ARIMA\n",
    "from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ValueWarning)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported.\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 1) PREPARE DATA\n",
    "# ===============================================================\n",
    "data = data.sort_index()\n",
    "\n",
    "if \"Return\" not in data.columns:\n",
    "    raise ValueError(\"The dataset must contain a column named 'Return'.\")\n",
    "\n",
    "if \"Direction\" not in data.columns:\n",
    "    raise ValueError(\"The dataset must contain a column named 'Direction'.\")\n",
    "\n",
    "print(\"Dataset loaded & sorted.\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 2) CREATE ARIMA FEATURE\n",
    "# ===============================================================\n",
    "print(\"Fitting ARIMA(2,1,2)...\")\n",
    "\n",
    "arima_model = ARIMA(data[\"Return\"], order=(2, 1, 2))\n",
    "arima_fit = arima_model.fit()\n",
    "\n",
    "forecast = arima_fit.predict(start=1, end=len(data), dynamic=False)\n",
    "forecast.index = data.index\n",
    "data[\"ARIMA_Return_Forecast\"] = forecast.shift(1)\n",
    "\n",
    "data = data.dropna(subset=[\"ARIMA_Return_Forecast\"])\n",
    "print(\"ARIMA_Return_Forecast feature created.\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 3) CLEAN FULL DATASET (NaN, inf, extreme values)\n",
    "# ===============================================================\n",
    "print(\"Cleaning NaN and infinite values...\")\n",
    "\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "data.fillna(method=\"ffill\", inplace=True)\n",
    "data.fillna(method=\"bfill\", inplace=True)\n",
    "\n",
    "# Cap extreme values (finance best practice)\n",
    "data = data.clip(lower=-1e6, upper=1e6)\n",
    "\n",
    "print(\"Cleaning completed.\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 4) FEATURE SELECTION (ANOVA — Top 20)\n",
    "# ===============================================================\n",
    "print(\"Running SelectKBest (ANOVA)...\")\n",
    "\n",
    "# Prepare features\n",
    "features = data.drop(columns=[\"Direction\"])\n",
    "target = data[\"Direction\"]\n",
    "\n",
    "# Standardize features for ANOVA\n",
    "scaler_fs = StandardScaler()\n",
    "X_scaled = scaler_fs.fit_transform(features)\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=20)\n",
    "selector.fit(X_scaled, target)\n",
    "\n",
    "ranking_df = pd.DataFrame({\n",
    "    \"Feature\": features.columns,\n",
    "    \"ANOVA_F_score\": selector.scores_,\n",
    "    \"p_value\": selector.pvalues_,\n",
    "    \"Selected\": selector.get_support()\n",
    "}).sort_values(by=\"ANOVA_F_score\", ascending=False)\n",
    "\n",
    "print(\"Top 20 features:\")\n",
    "top20_features = ranking_df.head(20)[\"Feature\"].tolist()\n",
    "print(top20_features)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 5) BUILD ML DATASET\n",
    "# ===============================================================\n",
    "X = data[top20_features]\n",
    "y = data[\"Direction\"]\n",
    "\n",
    "# Final cleaning before feeding models\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split (time series: no shuffling)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "print(\"Train/Test dataset ready.\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 6) MODEL SET\n",
    "# ===============================================================\n",
    "models = [\n",
    "    (\"LogReg\", LogisticRegression(max_iter=500)),\n",
    "    (\"RandomForest\", RandomForestClassifier(n_estimators=300)),\n",
    "    (\"ExtraTrees\", ExtraTreesClassifier(n_estimators=300)),\n",
    "    (\"GradientBoost\", GradientBoostingClassifier()),\n",
    "    (\"AdaBoost\", AdaBoostClassifier()),\n",
    "    (\"SVM\", SVC(probability=True)),\n",
    "    (\"KNN\", KNeighborsClassifier(n_neighbors=7)),\n",
    "    (\"MLP\", MLPClassifier(max_iter=500))\n",
    "]\n",
    "\n",
    "print(\"Models initialized.\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 7) TRAIN & EVALUATE\n",
    "# ===============================================================\n",
    "results = []\n",
    "\n",
    "for name, model in models:\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    results.append((name, acc, f1, auc))\n",
    "\n",
    "    print(f\"{name} — ACC: {acc:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 8) RESULTS TABLE\n",
    "# ===============================================================\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"F1\", \"ROC-AUC\"])\n",
    "print(\"\\nFinal Results:\")\n",
    "display(results_df.sort_values(by=\"ROC-AUC\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4e523d0-8a93-4345-a652-56f827853fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================\n",
      "   ADVANCED ML MODELS – ENSEMBLE & STACKING\n",
      "====================================================\n",
      "\n",
      "\n",
      "▶ Training Tuned SVM ...\n",
      "   Accuracy: 0.5390\n",
      "   F1-score: 0.6914\n",
      "   AUC: 0.4768\n",
      "\n",
      "▶ Training Tuned Decision Tree ...\n",
      "   Accuracy: 0.4761\n",
      "   F1-score: 0.4541\n",
      "   AUC: 0.4790\n",
      "\n",
      "▶ Training Bagging SVM ...\n",
      "   Accuracy: 0.5252\n",
      "   F1-score: 0.6887\n",
      "   AUC: 0.4826\n",
      "\n",
      "▶ Training Bagging Decision Tree ...\n",
      "   Accuracy: 0.5139\n",
      "   F1-score: 0.5246\n",
      "   AUC: 0.5100\n",
      "\n",
      "▶ Training Voting (SVM + DT) ...\n",
      "   Accuracy: 0.5063\n",
      "   F1-score: 0.5075\n",
      "   AUC: 0.4932\n",
      "\n",
      "▶ Training Stacking Meta-Model ...\n",
      "   Accuracy: 0.5139\n",
      "   F1-score: 0.5768\n",
      "   AUC: 0.5117\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tuned SVM</td>\n",
       "      <td>0.539043</td>\n",
       "      <td>0.691400</td>\n",
       "      <td>0.476779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bagging SVM</td>\n",
       "      <td>0.525189</td>\n",
       "      <td>0.688687</td>\n",
       "      <td>0.482619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stacking Meta-Model</td>\n",
       "      <td>0.513854</td>\n",
       "      <td>0.576754</td>\n",
       "      <td>0.511688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bagging Decision Tree</td>\n",
       "      <td>0.513854</td>\n",
       "      <td>0.524631</td>\n",
       "      <td>0.509974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Voting (SVM + DT)</td>\n",
       "      <td>0.506297</td>\n",
       "      <td>0.507538</td>\n",
       "      <td>0.493213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tuned Decision Tree</td>\n",
       "      <td>0.476071</td>\n",
       "      <td>0.454068</td>\n",
       "      <td>0.479047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Accuracy  F1-score       AUC\n",
       "0              Tuned SVM  0.539043  0.691400  0.476779\n",
       "2            Bagging SVM  0.525189  0.688687  0.482619\n",
       "5    Stacking Meta-Model  0.513854  0.576754  0.511688\n",
       "3  Bagging Decision Tree  0.513854  0.524631  0.509974\n",
       "4      Voting (SVM + DT)  0.506297  0.507538  0.493213\n",
       "1    Tuned Decision Tree  0.476071  0.454068  0.479047"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# ADVANCED MODELS – ENSEMBLE & META-LEARNING COMPARISON\n",
    "# ===============================================================\n",
    "print(\"\\n====================================================\")\n",
    "print(\"   ADVANCED ML MODELS – ENSEMBLE & STACKING\")\n",
    "print(\"====================================================\\n\")\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    BaggingClassifier,\n",
    "    VotingClassifier,\n",
    "    StackingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# PREPROCESSING PIPELINE (Imputation + Scaling)\n",
    "# ===============================================================\n",
    "def make_pipeline(model):\n",
    "    return Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1) BASE MODELS (for consistency)\n",
    "# ---------------------------------------------------------------\n",
    "base_svm = SVC(kernel='rbf', probability=True, random_state=0)\n",
    "base_dt = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 2) TUNED MODELS WITH GRID SEARCH (with Pipeline)\n",
    "# ===============================================================\n",
    "param_svm = {\n",
    "    \"model__C\": [0.1, 1, 10],\n",
    "    \"model__gamma\": [\"scale\", 0.01, 0.001]\n",
    "}\n",
    "\n",
    "tuned_svm = GridSearchCV(\n",
    "    make_pipeline(SVC(probability=True, random_state=0)),\n",
    "    param_svm,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    error_score=\"raise\"\n",
    ")\n",
    "tuned_svm.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "param_dt = {\n",
    "    \"model__max_depth\": [3, 5, 7, None],\n",
    "    \"model__min_samples_split\": [2, 5, 10]\n",
    "}\n",
    "\n",
    "tuned_dt = GridSearchCV(\n",
    "    make_pipeline(DecisionTreeClassifier(random_state=0)),\n",
    "    param_dt,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    error_score=\"raise\"\n",
    ")\n",
    "tuned_dt.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 3) BAGGING MODELS (corrected for scikit-learn ≥ 1.4)\n",
    "# ===============================================================\n",
    "\n",
    "bagging_svm = make_pipeline(\n",
    "    BaggingClassifier(\n",
    "        estimator=SVC(kernel='rbf', probability=True),\n",
    "        n_estimators=20,\n",
    "        max_samples=0.8,\n",
    "        random_state=0\n",
    "    )\n",
    ")\n",
    "\n",
    "bagging_dt = make_pipeline(\n",
    "    BaggingClassifier(\n",
    "        estimator=DecisionTreeClassifier(),\n",
    "        n_estimators=50,\n",
    "        max_samples=0.8,\n",
    "        random_state=0\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 4) VOTING CLASSIFIER (pipelined)\n",
    "# ===============================================================\n",
    "voting = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", VotingClassifier(\n",
    "        estimators=[\n",
    "            ('svm', SVC(kernel='rbf', probability=True)),\n",
    "            ('dt', DecisionTreeClassifier())\n",
    "        ],\n",
    "        voting='soft'\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 5) STACKING CLASSIFIER (with corrected Bagging)\n",
    "# ===============================================================\n",
    "stacking = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", StackingClassifier(\n",
    "        estimators=[\n",
    "            ('svm', SVC(kernel='rbf', probability=True)),\n",
    "            ('dt', DecisionTreeClassifier()),\n",
    "            ('bag_svm', BaggingClassifier(\n",
    "                estimator=SVC(kernel='rbf', probability=True),\n",
    "                n_estimators=10)),\n",
    "            ('bag_dt', BaggingClassifier(\n",
    "                estimator=DecisionTreeClassifier(),\n",
    "                n_estimators=10))\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(max_iter=500),\n",
    "        passthrough=True,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# TRAIN + EVALUATE ALL MODELS\n",
    "# ===============================================================\n",
    "models = {\n",
    "    \"Tuned SVM\": tuned_svm.best_estimator_,\n",
    "    \"Tuned Decision Tree\": tuned_dt.best_estimator_,\n",
    "    \"Bagging SVM\": bagging_svm,\n",
    "    \"Bagging Decision Tree\": bagging_dt,\n",
    "    \"Voting (SVM + DT)\": voting,\n",
    "    \"Stacking Meta-Model\": stacking\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n▶ Training {name} ...\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    results.append([name, acc, f1, auc])\n",
    "\n",
    "    print(f\"   Accuracy: {acc:.4f}\")\n",
    "    print(f\"   F1-score: {f1:.4f}\")\n",
    "    print(f\"   AUC: {auc:.4f}\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# SUMMARY TABLE\n",
    "# ===============================================================\n",
    "df_results = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"F1-score\", \"AUC\"])\n",
    "df_results.sort_values(by=\"F1-score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8285e2b7-9db8-4402-b361-f7ceabf3a712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               SARIMAX Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:                 Return   No. Observations:                 3967\n",
      "Model:                 ARIMA(2, 1, 2)   Log Likelihood               10423.761\n",
      "Date:                Wed, 19 Nov 2025   AIC                         -20837.522\n",
      "Time:                        09:18:55   BIC                         -20806.094\n",
      "Sample:                             0   HQIC                        -20826.377\n",
      "                               - 3967                                         \n",
      "Covariance Type:                  opg                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "ar.L1         -0.9839      0.060    -16.380      0.000      -1.102      -0.866\n",
      "ar.L2         -0.0397      0.010     -3.983      0.000      -0.059      -0.020\n",
      "ma.L1         -0.0494      0.059     -0.840      0.401      -0.165       0.066\n",
      "ma.L2         -0.9496      0.058    -16.249      0.000      -1.064      -0.835\n",
      "sigma2         0.0003   3.75e-06     81.192      0.000       0.000       0.000\n",
      "===================================================================================\n",
      "Ljung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):              6649.05\n",
      "Prob(Q):                              0.98   Prob(JB):                         0.00\n",
      "Heteroskedasticity (H):               1.17   Skew:                             0.04\n",
      "Prob(H) (two-sided):                  0.00   Kurtosis:                         9.34\n",
      "===================================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n",
      "Data shape after adding ARIMA feature and dropping NaNs: (3966, 26)\n",
      "Final feature columns used:\n",
      "['US10Y', 'LQD', 'HangSeng', 'IEF', 'DowJones', 'BND', 'TLT', 'Nikkei225', 'MSCIWorld', 'MA20', 'Imports_GDP_Pct', 'US2Y', 'Momentum', 'S&P500', 'Recession_Probability', 'Exports_GDP_Pct', 'DAX', 'Inflation_Annual_Pct', 'Fed_Funds_Rate', 'CAC40', 'target_index', 'ARIMA_Return_Forecast', 'ARIMA_Direction']\n",
      "Train shape: (3172, 23)  Test shape: (794, 23)\n",
      "\n",
      "Running classification models with ARIMA feature...\n",
      "\n",
      "LR: Accuracy=0.5189 | F1=0.6774 | ROC-AUC=0.5100\n",
      "KNN: Accuracy=0.4748 | F1=0.4549 | ROC-AUC=0.4651\n",
      "CART: Accuracy=0.4811 | F1=0.3419 | ROC-AUC=0.4964\n",
      "SVC: Accuracy=0.5239 | F1=0.6866 | ROC-AUC=0.4811\n",
      "MLP: Accuracy=0.4962 | F1=0.4382 | ROC-AUC=0.5166\n",
      "ABR: Accuracy=0.5063 | F1=0.5088 | ROC-AUC=0.4989\n",
      "GBR: Accuracy=0.4924 | F1=0.5287 | ROC-AUC=0.4965\n",
      "RFR: Accuracy=0.5038 | F1=0.5651 | ROC-AUC=0.4844\n",
      "ETR: Accuracy=0.5013 | F1=0.5406 | ROC-AUC=0.4877\n",
      "\n",
      "=========== Ranked Results (by ROC-AUC) ===========\n",
      "Model  Accuracy       F1  ROC_AUC\n",
      "  MLP  0.496222 0.438202 0.516631\n",
      "   LR  0.518892 0.677365 0.510047\n",
      "  ABR  0.506297 0.508772 0.498909\n",
      "  GBR  0.492443 0.528655 0.496511\n",
      " CART  0.481108 0.341853 0.496393\n",
      "  ETR  0.501259 0.540603 0.487714\n",
      "  RFR  0.503778 0.565121 0.484432\n",
      "  SVC  0.523929 0.686567 0.481149\n",
      "  KNN  0.474811 0.454902 0.465123\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# 0) IMPORTS\n",
    "# ===============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier, GradientBoostingClassifier,\n",
    "    RandomForestClassifier, ExtraTreesClassifier\n",
    ")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# ===============================================================\n",
    "# 1) SORT DATA & CHECK COLUMNS\n",
    "# ===============================================================\n",
    "# Make sure data is sorted in time\n",
    "data = data.sort_index()\n",
    "\n",
    "required_cols = [\"Return\", \"Direction\"]\n",
    "for c in required_cols:\n",
    "    if c not in data.columns:\n",
    "        raise ValueError(f\"Missing required column: {c}\")\n",
    "\n",
    "# ===============================================================\n",
    "# 2) BUILD ARIMA FORECAST FEATURE\n",
    "# ===============================================================\n",
    "# ARIMA on 'Return' – order (2,1,2) as we discussed\n",
    "arima_model = ARIMA(data[\"Return\"], order=(2, 1, 2))\n",
    "arima_fit = arima_model.fit()\n",
    "\n",
    "print(arima_fit.summary())  # optional\n",
    "\n",
    "# In-sample one-step-ahead predictions\n",
    "arima_pred = arima_fit.predict(start=1, end=len(data), dynamic=False)\n",
    "\n",
    "# Align index with data\n",
    "arima_pred.index = data.index\n",
    "\n",
    "# Shift by 1: forecast at t is based only on information up to t-1\n",
    "data[\"ARIMA_Return_Forecast\"] = arima_pred.shift(1)\n",
    "\n",
    "# You can also create a directional signal from ARIMA, optional:\n",
    "data[\"ARIMA_Direction\"] = (data[\"ARIMA_Return_Forecast\"] > 0).astype(int)\n",
    "\n",
    "# Because of the shift, first row is NaN → drop it (or more if needed)\n",
    "data_model = data.dropna(subset=[\"ARIMA_Return_Forecast\"]).copy()\n",
    "\n",
    "print(\"Data shape after adding ARIMA feature and dropping NaNs:\", data_model.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 3) DEFINE FEATURES & TARGET\n",
    "# ===============================================================\n",
    "# Drop raw price & return & target, keep everything else including ARIMA feature\n",
    "X = data_model.drop(columns=[\"Apple\", \"Return\", \"Direction\"])\n",
    "y = data_model[\"Direction\"]\n",
    "\n",
    "print(\"Final feature columns used:\")\n",
    "print(X.columns.tolist())\n",
    "\n",
    "# ===============================================================\n",
    "# 4) TRAIN / TEST SPLIT (TIME-SERIES FRIENDLY)\n",
    "# ===============================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \" Test shape:\", X_test.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 5) DEFINE CLASSIFICATION MODELS\n",
    "# ===============================================================\n",
    "models = [\n",
    "    ('LR',  LogisticRegression(max_iter=5000, random_state=RANDOM_STATE)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('CART', DecisionTreeClassifier(max_depth=6, random_state=RANDOM_STATE)),\n",
    "    ('SVC', SVC(kernel='rbf', probability=True, random_state=RANDOM_STATE)),\n",
    "    ('MLP', MLPClassifier(hidden_layer_sizes=(64, 32),\n",
    "                          max_iter=5000, random_state=RANDOM_STATE)),\n",
    "    # Boosting\n",
    "    ('ABR', AdaBoostClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('GBR', GradientBoostingClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    # Bagging\n",
    "    ('RFR', RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('ETR', ExtraTreesClassifier(n_estimators=300, random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "# ===============================================================\n",
    "# 6) EVALUATE MODELS WITH ARIMA FEATURE\n",
    "# ===============================================================\n",
    "results = []\n",
    "\n",
    "print(\"\\nRunning classification models with ARIMA feature...\\n\")\n",
    "\n",
    "for name, model in models:\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1  = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    results.append([name, acc, f1, auc])\n",
    "\n",
    "    print(f\"{name}: \"\n",
    "          f\"Accuracy={acc:.4f} | F1={f1:.4f} | ROC-AUC={auc:.4f}\")\n",
    "\n",
    "# ===============================================================\n",
    "# 7) RESULTS TABLE\n",
    "# ===============================================================\n",
    "results_df = pd.DataFrame(\n",
    "    results, columns=[\"Model\", \"Accuracy\", \"F1\", \"ROC_AUC\"]\n",
    ").sort_values(by=\"ROC_AUC\", ascending=False)\n",
    "\n",
    "print(\"\\n=========== Ranked Results (by ROC-AUC) ===========\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25d80a88-efa5-49e6-bb82-95f43a6594b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM input shape: (3947, 20, 1)\n",
      "Epoch 1/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 3.1124e-04 - val_loss: 3.6382e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.0916e-04 - val_loss: 3.8546e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 3.0670e-04 - val_loss: 3.7087e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.0614e-04 - val_loss: 3.6176e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.0874e-04 - val_loss: 3.6164e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.0698e-04 - val_loss: 3.6220e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 3.0538e-04 - val_loss: 3.6222e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.0567e-04 - val_loss: 3.6252e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 3.0750e-04 - val_loss: 3.6138e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.0646e-04 - val_loss: 3.8014e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.0623e-04 - val_loss: 3.6130e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.0621e-04 - val_loss: 3.6557e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.0719e-04 - val_loss: 3.6058e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.0560e-04 - val_loss: 3.7334e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.0665e-04 - val_loss: 3.6111e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.0704e-04 - val_loss: 3.6458e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 3.0643e-04 - val_loss: 3.6244e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.0518e-04 - val_loss: 3.6094e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.0575e-04 - val_loss: 3.6068e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.0466e-04 - val_loss: 3.6475e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.0439e-04 - val_loss: 3.6907e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.0458e-04 - val_loss: 3.6145e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.0513e-04 - val_loss: 3.7395e-04\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "Data shape after adding LSTM feature: (3946, 28)\n",
      "\n",
      "Running ML classifiers WITH LSTM feature...\n",
      "\n",
      "LR: Accuracy=0.5241 | F1=0.6840 | ROC-AUC=0.5130\n",
      "KNN: Accuracy=0.4886 | F1=0.4584 | ROC-AUC=0.4842\n",
      "CART: Accuracy=0.4899 | F1=0.3994 | ROC-AUC=0.5013\n",
      "SVC: Accuracy=0.5228 | F1=0.6856 | ROC-AUC=0.4775\n",
      "MLP: Accuracy=0.5481 | F1=0.6584 | ROC-AUC=0.5285\n",
      "ABR: Accuracy=0.4759 | F1=0.4926 | ROC-AUC=0.4853\n",
      "GBR: Accuracy=0.5089 | F1=0.5561 | ROC-AUC=0.4944\n",
      "RFR: Accuracy=0.5127 | F1=0.5783 | ROC-AUC=0.5095\n",
      "ETR: Accuracy=0.5051 | F1=0.5727 | ROC-AUC=0.4969\n",
      "\n",
      "=========== Ranked Results (LSTM-enhanced ML) ===========\n",
      "Model  Accuracy       F1  ROC_AUC\n",
      "  MLP  0.548101 0.658373 0.528510\n",
      "   LR  0.524051 0.684034 0.513041\n",
      "  RFR  0.512658 0.578313 0.509537\n",
      " CART  0.489873 0.399404 0.501262\n",
      "  ETR  0.505063 0.572678 0.496881\n",
      "  GBR  0.508861 0.556064 0.494366\n",
      "  ABR  0.475949 0.492647 0.485302\n",
      "  KNN  0.488608 0.458445 0.484206\n",
      "  SVC  0.522785 0.685571 0.477490\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# 0) IMPORTS\n",
    "# ===============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# ML Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier, GradientBoostingClassifier,\n",
    "    RandomForestClassifier, ExtraTreesClassifier\n",
    ")\n",
    "\n",
    "# Deep Learning\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# ===============================================================\n",
    "# 1) SORT DATA\n",
    "# ===============================================================\n",
    "data = data.sort_index()\n",
    "\n",
    "# ===============================================================\n",
    "# 2) BUILD SUPERVISED SEQUENCE DATA FOR LSTM\n",
    "#    Here we use past 20 returns to predict return(t+1)\n",
    "# ===============================================================\n",
    "SEQ_LEN = 20\n",
    "\n",
    "returns = data[\"Return\"].values.reshape(-1, 1)\n",
    "\n",
    "X_seq = []\n",
    "y_seq = []\n",
    "\n",
    "for i in range(SEQ_LEN, len(returns)):\n",
    "    X_seq.append(returns[i-SEQ_LEN:i])\n",
    "    y_seq.append(returns[i])\n",
    "\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "print(\"LSTM input shape:\", X_seq.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 3) TRAIN/TEST SPLIT (no shuffle)\n",
    "# ===============================================================\n",
    "split = int(0.8 * len(X_seq))\n",
    "\n",
    "X_train_seq = X_seq[:split]\n",
    "X_test_seq  = X_seq[split:]\n",
    "\n",
    "y_train_seq = y_seq[:split]\n",
    "y_test_seq  = y_seq[split:]\n",
    "\n",
    "# ===============================================================\n",
    "# 4) DEFINE LSTM MODEL\n",
    "# ===============================================================\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(SEQ_LEN, 1)),\n",
    "    LSTM(32),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# ===============================================================\n",
    "# 5) TRAIN LSTM\n",
    "# ===============================================================\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_split=0.1,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ===============================================================\n",
    "# 6) FORECAST RETURNS WITH LSTM\n",
    "# ===============================================================\n",
    "lstm_forecast = model.predict(X_seq).flatten()\n",
    "\n",
    "# Shift by 1 to avoid lookahead bias\n",
    "lstm_forecast = pd.Series(lstm_forecast, index=data.index[SEQ_LEN:])\n",
    "lstm_forecast = lstm_forecast.shift(1)\n",
    "\n",
    "data[\"LSTM_Return_Forecast\"] = lstm_forecast\n",
    "\n",
    "# Optional: binary directional prediction from LSTM\n",
    "data[\"LSTM_Direction\"] = (data[\"LSTM_Return_Forecast\"] > 0).astype(int)\n",
    "\n",
    "# Drop the initial NaNs created by LSTM sequence length\n",
    "data_ml = data.dropna(subset=[\"LSTM_Return_Forecast\"])\n",
    "\n",
    "print(\"Data shape after adding LSTM feature:\", data_ml.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 7) PREPARE ML CLASSIFICATION DATA\n",
    "# ===============================================================\n",
    "X = data_ml.drop(columns=[\"Apple\", \"Return\", \"Direction\"])\n",
    "y = data_ml[\"Direction\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# ===============================================================\n",
    "# 8) DEFINE ML MODELS\n",
    "# ===============================================================\n",
    "models = [\n",
    "    ('LR',  LogisticRegression(max_iter=5000, random_state=RANDOM_STATE)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('CART', DecisionTreeClassifier(max_depth=6, random_state=RANDOM_STATE)),\n",
    "    ('SVC', SVC(kernel='rbf', probability=True, random_state=RANDOM_STATE)),\n",
    "    ('MLP', MLPClassifier(hidden_layer_sizes=(64, 32),\n",
    "                          max_iter=5000, random_state=RANDOM_STATE)),\n",
    "    ('ABR', AdaBoostClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('GBR', GradientBoostingClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('RFR', RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('ETR', ExtraTreesClassifier(n_estimators=300, random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "# ===============================================================\n",
    "# 9) EVALUATE THE ML CLASSIFIERS (WITH LSTM FEATURE)\n",
    "# ===============================================================\n",
    "results = []\n",
    "\n",
    "print(\"\\nRunning ML classifiers WITH LSTM feature...\\n\")\n",
    "\n",
    "for name, model_ml in models:\n",
    "    model_ml.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model_ml.predict(X_test)\n",
    "    y_proba = model_ml.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1  = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    results.append([name, acc, f1, auc])\n",
    "\n",
    "    print(f\"{name}: Accuracy={acc:.4f} | F1={f1:.4f} | ROC-AUC={auc:.4f}\")\n",
    "\n",
    "# ===============================================================\n",
    "# 10) RESULTS TABLE\n",
    "# ===============================================================\n",
    "results_df = pd.DataFrame(\n",
    "    results, columns=[\"Model\", \"Accuracy\", \"F1\", \"ROC_AUC\"]\n",
    ").sort_values(by=\"ROC_AUC\", ascending=False)\n",
    "\n",
    "print(\"\\n=========== Ranked Results (LSTM-enhanced ML) ===========\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9cc9592-6229-4e53-881f-05e09d84251a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ARIMA(2,1,2) on returns...\n",
      "Building LSTM sequences...\n",
      "LSTM input shape: (3947, 20, 1)\n",
      "Training LSTM...\n",
      "Epoch 1/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 3.1823e-04 - val_loss: 3.6345e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.1153e-04 - val_loss: 3.6941e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.0868e-04 - val_loss: 3.6249e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 3.1008e-04 - val_loss: 3.6473e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.0577e-04 - val_loss: 3.6076e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.0680e-04 - val_loss: 3.6114e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 3.0655e-04 - val_loss: 3.6250e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.0601e-04 - val_loss: 3.6110e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.0541e-04 - val_loss: 3.6271e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.0662e-04 - val_loss: 3.6050e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.0810e-04 - val_loss: 3.6265e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.0553e-04 - val_loss: 3.6060e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.0790e-04 - val_loss: 3.6600e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.0566e-04 - val_loss: 3.6564e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.0675e-04 - val_loss: 3.6356e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 3.0678e-04 - val_loss: 3.6186e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.0551e-04 - val_loss: 3.6042e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.0521e-04 - val_loss: 3.6715e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.0651e-04 - val_loss: 3.7945e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.0622e-04 - val_loss: 3.6112e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.0536e-04 - val_loss: 3.6115e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.0592e-04 - val_loss: 3.6627e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.0612e-04 - val_loss: 3.6543e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.0684e-04 - val_loss: 3.7005e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.0723e-04 - val_loss: 3.6229e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.0469e-04 - val_loss: 3.6432e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.0512e-04 - val_loss: 3.6605e-04\n",
      "Generating LSTM forecasts...\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "Data shape after ARIMA+LSTM features: (3946, 28)\n",
      "Train shape: (3156, 25) Test shape: (790, 25)\n",
      "Base feature count (excluding ARIMA/LSTM): 23\n",
      "\n",
      "Building out-of-fold meta-features (stacking)...\n",
      "  OOF for base model: LR\n",
      "  OOF for base model: KNN\n",
      "  OOF for base model: CART\n",
      "  OOF for base model: SVC\n",
      "  OOF for base model: MLP\n",
      "  OOF for base model: ABR\n",
      "  OOF for base model: GBR\n",
      "  OOF for base model: RFR\n",
      "  OOF for base model: ETR\n",
      "Final meta_features_train shape: (3156, 11)\n",
      "\n",
      "Building meta-features for TEST set...\n",
      "  Train full + predict test for: LR\n",
      "  Train full + predict test for: KNN\n",
      "  Train full + predict test for: CART\n",
      "  Train full + predict test for: SVC\n",
      "  Train full + predict test for: MLP\n",
      "  Train full + predict test for: ABR\n",
      "  Train full + predict test for: GBR\n",
      "  Train full + predict test for: RFR\n",
      "  Train full + predict test for: ETR\n",
      "Final meta_features_test shape: (790, 11)\n",
      "\n",
      "=========== META-MODEL PERFORMANCE (ARIMA + LSTM + ML) ===========\n",
      "Accuracy = 0.5089\n",
      "F1       = 0.5280\n",
      "ROC-AUC  = 0.4976\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# 0) IMPORTS\n",
    "# ===============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier, GradientBoostingClassifier,\n",
    "    RandomForestClassifier, ExtraTreesClassifier\n",
    ")\n",
    "\n",
    "# Deep learning (LSTM)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# ===============================================================\n",
    "# 1) DATA PREP: SORT + BASIC CHECKS\n",
    "# ===============================================================\n",
    "# Assume 'data' already exists with 'Return' and 'Direction'\n",
    "data = data.sort_index()\n",
    "\n",
    "if \"Return\" not in data.columns or \"Direction\" not in data.columns:\n",
    "    raise ValueError(\"Data must contain 'Return' and 'Direction' columns.\")\n",
    "\n",
    "# ===============================================================\n",
    "# 2) ARIMA FORECAST: ARIMA_Return_Forecast\n",
    "# ===============================================================\n",
    "print(\"Fitting ARIMA(2,1,2) on returns...\")\n",
    "\n",
    "arima_model = ARIMA(data[\"Return\"], order=(2, 1, 2))\n",
    "arima_fit = arima_model.fit()\n",
    "\n",
    "arima_pred = arima_fit.predict(start=1, end=len(data), dynamic=False)\n",
    "arima_pred.index = data.index\n",
    "data[\"ARIMA_Return_Forecast\"] = arima_pred.shift(1)  # avoid look-ahead\n",
    "\n",
    "# Optional directional signal from ARIMA\n",
    "data[\"ARIMA_Direction\"] = (data[\"ARIMA_Return_Forecast\"] > 0).astype(int)\n",
    "\n",
    "# ===============================================================\n",
    "# 3) LSTM FORECAST: LSTM_Return_Forecast\n",
    "# ===============================================================\n",
    "print(\"Building LSTM sequences...\")\n",
    "\n",
    "SEQ_LEN = 20\n",
    "returns = data[\"Return\"].values.reshape(-1, 1)\n",
    "\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(SEQ_LEN, len(returns)):\n",
    "    X_seq.append(returns[i-SEQ_LEN:i])\n",
    "    y_seq.append(returns[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "print(\"LSTM input shape:\", X_seq.shape)\n",
    "\n",
    "# Split for LSTM (purely for forecasting, still time-ordered)\n",
    "split_idx = int(0.8 * len(X_seq))\n",
    "X_train_seq, X_test_seq = X_seq[:split_idx], X_seq[split_idx:]\n",
    "y_train_seq, y_test_seq = y_seq[:split_idx], y_seq[split_idx:]\n",
    "\n",
    "# Define LSTM\n",
    "model_lstm = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(SEQ_LEN, 1)),\n",
    "    LSTM(32),\n",
    "    Dense(1)\n",
    "])\n",
    "model_lstm.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "print(\"Training LSTM...\")\n",
    "history = model_lstm.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_split=0.1,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Generating LSTM forecasts...\")\n",
    "lstm_forecast = model_lstm.predict(X_seq).flatten()\n",
    "\n",
    "# Align with dates: first forecast corresponds to index[SEQ_LEN:]\n",
    "lstm_series = pd.Series(lstm_forecast, index=data.index[SEQ_LEN:])\n",
    "# Shift by 1 to avoid look-ahead\n",
    "lstm_series = lstm_series.shift(1)\n",
    "\n",
    "data[\"LSTM_Return_Forecast\"] = lstm_series\n",
    "data[\"LSTM_Direction\"] = (data[\"LSTM_Return_Forecast\"] > 0).astype(int)\n",
    "\n",
    "# ===============================================================\n",
    "# 4) DROP NaNs CREATED BY ARIMA/LSTM\n",
    "# ===============================================================\n",
    "data_ml = data.dropna(subset=[\"ARIMA_Return_Forecast\", \"LSTM_Return_Forecast\"]).copy()\n",
    "data_ml = data_ml.sort_index()\n",
    "\n",
    "print(\"Data shape after ARIMA+LSTM features:\", data_ml.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 5) FEATURES & TARGET FOR ML\n",
    "# ===============================================================\n",
    "X_full = data_ml.drop(columns=[\"Apple\", \"Return\", \"Direction\"])\n",
    "y_full = data_ml[\"Direction\"]\n",
    "\n",
    "# Time-safe train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "\n",
    "# Keep ARIMA/LSTM features separately for meta-layer\n",
    "arima_train = X_train[\"ARIMA_Return_Forecast\"].values.reshape(-1, 1)\n",
    "lstm_train  = X_train[\"LSTM_Return_Forecast\"].values.reshape(-1, 1)\n",
    "\n",
    "arima_test = X_test[\"ARIMA_Return_Forecast\"].values.reshape(-1, 1)\n",
    "lstm_test  = X_test[\"LSTM_Return_Forecast\"].values.reshape(-1, 1)\n",
    "\n",
    "# Base ML features without ARIMA/LSTM if you want to avoid duplicating them\n",
    "base_feature_cols = [c for c in X_train.columns\n",
    "                     if c not in [\"ARIMA_Return_Forecast\", \"LSTM_Return_Forecast\"]]\n",
    "X_train_base = X_train[base_feature_cols]\n",
    "X_test_base  = X_test[base_feature_cols]\n",
    "\n",
    "print(\"Base feature count (excluding ARIMA/LSTM):\", len(base_feature_cols))\n",
    "\n",
    "# ===============================================================\n",
    "# 6) DEFINE BASE CLASSIFICATION MODELS\n",
    "# ===============================================================\n",
    "base_models = [\n",
    "    ('LR',  LogisticRegression(max_iter=5000, random_state=RANDOM_STATE)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('CART', DecisionTreeClassifier(max_depth=6, random_state=RANDOM_STATE)),\n",
    "    ('SVC', SVC(kernel='rbf', probability=True, random_state=RANDOM_STATE)),\n",
    "    ('MLP', MLPClassifier(hidden_layer_sizes=(64, 32),\n",
    "                          max_iter=5000, random_state=RANDOM_STATE)),\n",
    "    ('ABR', AdaBoostClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('GBR', GradientBoostingClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('RFR', RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('ETR', ExtraTreesClassifier(n_estimators=300, random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "# ===============================================================\n",
    "# 7) STACKING – BUILD OUT-OF-FOLD META-FEATURES (TRAIN SET)\n",
    "# ===============================================================\n",
    "print(\"\\nBuilding out-of-fold meta-features (stacking)...\")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "n_train = X_train_base.shape[0]\n",
    "n_models = len(base_models)\n",
    "\n",
    "# Only base models here: shape (n_train, n_models)\n",
    "meta_train_base = np.full((n_train, n_models), np.nan)\n",
    "\n",
    "for m_idx, (name, model) in enumerate(base_models):\n",
    "    print(f\"  OOF for base model: {name}\")\n",
    "\n",
    "    oof_pred = np.full(n_train, np.nan)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train_base)):\n",
    "        X_tr, X_val = X_train_base.iloc[train_idx], X_train_base.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        proba_val = model.predict_proba(X_val)[:, 1]\n",
    "        oof_pred[val_idx] = proba_val\n",
    "\n",
    "    # Now fill missing OOF predictions (earlier samples never validated)\n",
    "    if np.isnan(oof_pred).any():\n",
    "        model.fit(X_train_base, y_train)\n",
    "        full_pred = model.predict_proba(X_train_base)[:, 1]\n",
    "        oof_pred = np.where(np.isnan(oof_pred), full_pred, oof_pred)\n",
    "\n",
    "    meta_train_base[:, m_idx] = oof_pred\n",
    "\n",
    "# Combine with ARIMA and LSTM features\n",
    "meta_features_train = np.concatenate(\n",
    "    [meta_train_base, arima_train, lstm_train], axis=1\n",
    ")\n",
    "\n",
    "print(\"Final meta_features_train shape:\", meta_features_train.shape)  # MUST be (N, 11)\n",
    "\n",
    "# ===============================================================\n",
    "# 8) TRAIN META MODEL\n",
    "# ===============================================================\n",
    "meta_clf = LogisticRegression(max_iter=5000, random_state=RANDOM_STATE)\n",
    "meta_clf.fit(meta_features_train, y_train)\n",
    "\n",
    "# ===============================================================\n",
    "# 9) BUILD META-FEATURES FOR TEST SET\n",
    "# ===============================================================\n",
    "print(\"\\nBuilding meta-features for TEST set...\")\n",
    "\n",
    "meta_test_base = np.zeros((X_test_base.shape[0], n_models))\n",
    "\n",
    "for m_idx, (name, model) in enumerate(base_models):\n",
    "    print(f\"  Train full + predict test for: {name}\")\n",
    "    model.fit(X_train_base, y_train)\n",
    "    meta_test_base[:, m_idx] = model.predict_proba(X_test_base)[:, 1]\n",
    "\n",
    "# Add ARIMA + LSTM\n",
    "meta_features_test = np.concatenate(\n",
    "    [meta_test_base, arima_test, lstm_test], axis=1\n",
    ")\n",
    "\n",
    "print(\"Final meta_features_test shape:\", meta_features_test.shape)  # MUST be (N_test, 11)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 10) EVALUATE META-MODEL\n",
    "# ===============================================================\n",
    "y_pred_meta = meta_clf.predict(meta_features_test)\n",
    "y_proba_meta = meta_clf.predict_proba(meta_features_test)[:, 1]\n",
    "\n",
    "acc_meta = accuracy_score(y_test, y_pred_meta)\n",
    "f1_meta  = f1_score(y_test, y_pred_meta)\n",
    "auc_meta = roc_auc_score(y_test, y_proba_meta)\n",
    "\n",
    "print(\"\\n=========== META-MODEL PERFORMANCE (ARIMA + LSTM + ML) ===========\")\n",
    "print(f\"Accuracy = {acc_meta:.4f}\")\n",
    "print(f\"F1       = {f1_meta:.4f}\")\n",
    "print(f\"ROC-AUC  = {auc_meta:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dae2410b-3f6f-44f4-adb2-2965277945b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after building Trend_8D target: (2891, 30)\n",
      "Fitting ARIMA(2,1,2) on Return...\n",
      "Preparing LSTM sequences...\n",
      "LSTM input shape: (2871, 20, 1)\n",
      "Training LSTM...\n",
      "Epoch 1/50\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 3.3400e-04 - val_loss: 4.5569e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.1933e-04 - val_loss: 4.4459e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.2225e-04 - val_loss: 4.4249e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.1625e-04 - val_loss: 4.8252e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.1859e-04 - val_loss: 4.4331e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.1502e-04 - val_loss: 4.5110e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.1595e-04 - val_loss: 4.4388e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.1505e-04 - val_loss: 4.4358e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 3.1594e-04 - val_loss: 4.4342e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.1494e-04 - val_loss: 4.4290e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 3.1593e-04 - val_loss: 4.4463e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.1345e-04 - val_loss: 4.5081e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.1481e-04 - val_loss: 4.4500e-04\n",
      "Forecasting with LSTM...\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "Data shape after ARIMA+LSTM dropna: (2870, 30)\n",
      "Base feature count (excluding ARIMA/LSTM): 24\n",
      "\n",
      "Building out-of-fold meta-features for Trend_8D...\n",
      "\n",
      "  LR OOF predictions...\n",
      "  KNN OOF predictions...\n",
      "  CART OOF predictions...\n",
      "  SVC OOF predictions...\n",
      "  MLP OOF predictions...\n",
      "  ABR OOF predictions...\n",
      "  GBR OOF predictions...\n",
      "  RFR OOF predictions...\n",
      "  ETR OOF predictions...\n",
      "meta_features_train shape: (2296, 11)\n",
      "\n",
      "Building meta-features for TEST...\n",
      "\n",
      "  LR full-train → test prediction...\n",
      "  KNN full-train → test prediction...\n",
      "  CART full-train → test prediction...\n",
      "  SVC full-train → test prediction...\n",
      "  MLP full-train → test prediction...\n",
      "  ABR full-train → test prediction...\n",
      "  GBR full-train → test prediction...\n",
      "  RFR full-train → test prediction...\n",
      "  ETR full-train → test prediction...\n",
      "meta_features_test shape: (574, 11)\n",
      "\n",
      "=========== META-MODEL PERFORMANCE on Trend_8D (raw) ===========\n",
      "Accuracy = 1.0000\n",
      "F1       = 1.0000\n",
      "ROC-AUC  = 1.0000\n",
      "\n",
      "Confusion matrix:\n",
      "[[229   0]\n",
      " [  0 345]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000       229\n",
      "           1     1.0000    1.0000    1.0000       345\n",
      "\n",
      "    accuracy                         1.0000       574\n",
      "   macro avg     1.0000    1.0000    1.0000       574\n",
      "weighted avg     1.0000    1.0000    1.0000       574\n",
      "\n",
      "\n",
      "=========== META-MODEL PERFORMANCE with 8-day SMOOTHED forecast ===========\n",
      "Accuracy = 0.7173\n",
      "F1       = 0.7626\n",
      "ROC-AUC  = 0.7889\n",
      "\n",
      "Confusion matrix (smoothed):\n",
      "[[149  72]\n",
      " [ 88 257]]\n",
      "\n",
      "Classification report (smoothed):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6287    0.6742    0.6507       221\n",
      "           1     0.7812    0.7449    0.7626       345\n",
      "\n",
      "    accuracy                         0.7173       566\n",
      "   macro avg     0.7049    0.7096    0.7066       566\n",
      "weighted avg     0.7216    0.7173    0.7189       566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# 0) IMPORTS\n",
    "# ===============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier, GradientBoostingClassifier,\n",
    "    RandomForestClassifier, ExtraTreesClassifier\n",
    ")\n",
    "\n",
    "# Deep learning (LSTM)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# ===============================================================\n",
    "# 1) PREP DATA + CREATE 8-DAY FORWARD TREND TARGET\n",
    "# ===============================================================\n",
    "data = data.sort_index()\n",
    "\n",
    "required_cols = [\"Apple\", \"Return\", \"Direction\"]\n",
    "for c in required_cols:\n",
    "    if c not in data.columns:\n",
    "        raise ValueError(f\"Missing column: {c}\")\n",
    "\n",
    "# Forward-looking 8-day average of daily Direction\n",
    "# Direction_Forward8(t) = mean(Direction(t+1..t+8))\n",
    "data[\"Direction_Forward8\"] = (\n",
    "    data[\"Direction\"]\n",
    "    .rolling(8)\n",
    "    .mean()\n",
    "    .shift(-8)\n",
    ")\n",
    "\n",
    "# Binary trend label:\n",
    "#  1 if > 60% of future 8 days are up\n",
    "#  0 if < 40% of future 8 days are up\n",
    "#  NaN in between (neutral, we drop them for now)\n",
    "data[\"Trend_8D\"] = np.where(\n",
    "    data[\"Direction_Forward8\"] >= 0.6, 1,\n",
    "    np.where(data[\"Direction_Forward8\"] <= 0.4, 0, np.nan)\n",
    ")\n",
    "\n",
    "# Drop rows where target is NaN (start and end)\n",
    "data = data.dropna(subset=[\"Trend_8D\"]).copy()\n",
    "data[\"Trend_8D\"] = data[\"Trend_8D\"].astype(int)\n",
    "\n",
    "print(\"Data shape after building Trend_8D target:\", data.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 2) ARIMA FORECAST FEATURE (ON RETURN)\n",
    "# ===============================================================\n",
    "print(\"Fitting ARIMA(2,1,2) on Return...\")\n",
    "\n",
    "arima_model = ARIMA(data[\"Return\"], order=(2, 1, 2))\n",
    "arima_fit = arima_model.fit()\n",
    "\n",
    "arima_pred = arima_fit.predict(start=1, end=len(data), dynamic=False)\n",
    "arima_pred.index = data.index\n",
    "\n",
    "# Shift one step to avoid look-ahead\n",
    "data[\"ARIMA_Return_Forecast\"] = arima_pred.shift(1)\n",
    "data[\"ARIMA_Direction\"] = (data[\"ARIMA_Return_Forecast\"] > 0).astype(int)\n",
    "\n",
    "# ===============================================================\n",
    "# 3) LSTM FORECAST FEATURE (ON RETURN)\n",
    "# ===============================================================\n",
    "print(\"Preparing LSTM sequences...\")\n",
    "\n",
    "SEQ_LEN = 20\n",
    "returns_arr = data[\"Return\"].values.reshape(-1, 1)\n",
    "\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(SEQ_LEN, len(returns_arr)):\n",
    "    X_seq.append(returns_arr[i-SEQ_LEN:i])\n",
    "    y_seq.append(returns_arr[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "print(\"LSTM input shape:\", X_seq.shape)\n",
    "\n",
    "split_idx = int(0.8 * len(X_seq))\n",
    "X_train_seq, X_test_seq = X_seq[:split_idx], X_seq[split_idx:]\n",
    "y_train_seq, y_test_seq = y_seq[:split_idx], y_seq[split_idx:]\n",
    "\n",
    "model_lstm = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(SEQ_LEN, 1)),\n",
    "    LSTM(32),\n",
    "    Dense(1)\n",
    "])\n",
    "model_lstm.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "print(\"Training LSTM...\")\n",
    "model_lstm.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_split=0.1,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Forecasting with LSTM...\")\n",
    "lstm_pred = model_lstm.predict(X_seq).flatten()\n",
    "lstm_series = pd.Series(lstm_pred, index=data.index[SEQ_LEN:])\n",
    "\n",
    "# Shift by 1 to avoid look-ahead\n",
    "data[\"LSTM_Return_Forecast\"] = lstm_series.shift(1)\n",
    "data[\"LSTM_Direction\"] = (data[\"LSTM_Return_Forecast\"] > 0).astype(int)\n",
    "\n",
    "# ===============================================================\n",
    "# 4) DROP NaNs (from ARIMA/LSTM shifts and sequence start)\n",
    "# ===============================================================\n",
    "data_ml = data.dropna(subset=[\"ARIMA_Return_Forecast\", \"LSTM_Return_Forecast\"]).copy()\n",
    "print(\"Data shape after ARIMA+LSTM dropna:\", data_ml.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 5) FEATURES & TARGET FOR STACKING (Option C)\n",
    "# ===============================================================\n",
    "# Use ALL available predictors except:\n",
    "#  - raw price/return\n",
    "#  - daily Direction\n",
    "#  - forward-label helpers\n",
    "X_full = data_ml.drop(columns=[\n",
    "    \"Apple\", \"Return\", \"Direction\", \"Direction_Forward8\"\n",
    "])\n",
    "y_full = data_ml[\"Trend_8D\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# Keep ARIMA/LSTM forecasts separate for meta-layer\n",
    "arima_train = X_train[\"ARIMA_Return_Forecast\"].values.reshape(-1, 1)\n",
    "lstm_train  = X_train[\"LSTM_Return_Forecast\"].values.reshape(-1, 1)\n",
    "arima_test  = X_test[\"ARIMA_Return_Forecast\"].values.reshape(-1, 1)\n",
    "lstm_test   = X_test[\"LSTM_Return_Forecast\"].values.reshape(-1, 1)\n",
    "\n",
    "# Base feature set excludes ARIMA/LSTM forecasts\n",
    "base_feature_cols = [\n",
    "    c for c in X_train.columns\n",
    "    if c not in [\"ARIMA_Return_Forecast\", \"LSTM_Return_Forecast\"]\n",
    "]\n",
    "X_train_base = X_train[base_feature_cols]\n",
    "X_test_base  = X_test[base_feature_cols]\n",
    "\n",
    "print(\"Base feature count (excluding ARIMA/LSTM):\", len(base_feature_cols))\n",
    "\n",
    "# ===============================================================\n",
    "# 6) DEFINE BASE CLASSIFICATION MODELS\n",
    "# ===============================================================\n",
    "base_models = [\n",
    "    ('LR',  LogisticRegression(max_iter=5000, random_state=RANDOM_STATE)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('CART', DecisionTreeClassifier(max_depth=6, random_state=RANDOM_STATE)),\n",
    "    ('SVC',  SVC(kernel='rbf', probability=True, random_state=RANDOM_STATE)),\n",
    "    ('MLP',  MLPClassifier(hidden_layer_sizes=(64, 32),\n",
    "                           max_iter=5000, random_state=RANDOM_STATE)),\n",
    "    ('ABR',  AdaBoostClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('GBR',  GradientBoostingClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('RFR',  RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('ETR',  ExtraTreesClassifier(n_estimators=300, random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "# ===============================================================\n",
    "# 7) STACKING – BUILD OUT-OF-FOLD META-FEATURES (TRAIN)\n",
    "# ===============================================================\n",
    "print(\"\\nBuilding out-of-fold meta-features for Trend_8D...\\n\")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "n_train = X_train_base.shape[0]\n",
    "n_models = len(base_models)\n",
    "\n",
    "meta_train_base = np.full((n_train, n_models), np.nan)\n",
    "\n",
    "for m_idx, (name, model) in enumerate(base_models):\n",
    "    print(f\"  {name} OOF predictions...\")\n",
    "    oof_pred = np.full(n_train, np.nan)\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(tscv.split(X_train_base)):\n",
    "        X_tr, X_val = X_train_base.iloc[tr_idx], X_train_base.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        oof_pred[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Fill NaNs (early samples not in any validation fold)\n",
    "    if np.isnan(oof_pred).any():\n",
    "        model.fit(X_train_base, y_train)\n",
    "        full_pred = model.predict_proba(X_train_base)[:, 1]\n",
    "        oof_pred = np.where(np.isnan(oof_pred), full_pred, oof_pred)\n",
    "\n",
    "    meta_train_base[:, m_idx] = oof_pred\n",
    "\n",
    "# Append ARIMA & LSTM forecasts to meta-features\n",
    "meta_features_train = np.concatenate(\n",
    "    [meta_train_base, arima_train, lstm_train],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"meta_features_train shape:\", meta_features_train.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 8) TRAIN META-CLASSIFIER (LOGISTIC REGRESSION)\n",
    "# ===============================================================\n",
    "meta_clf = LogisticRegression(max_iter=5000, random_state=RANDOM_STATE)\n",
    "meta_clf.fit(meta_features_train, y_train)\n",
    "\n",
    "# ===============================================================\n",
    "# 9) META-FEATURES FOR TEST\n",
    "# ===============================================================\n",
    "print(\"\\nBuilding meta-features for TEST...\\n\")\n",
    "\n",
    "meta_test_base = np.zeros((X_test_base.shape[0], n_models))\n",
    "\n",
    "for m_idx, (name, model) in enumerate(base_models):\n",
    "    print(f\"  {name} full-train → test prediction...\")\n",
    "    model.fit(X_train_base, y_train)\n",
    "    meta_test_base[:, m_idx] = model.predict_proba(X_test_base)[:, 1]\n",
    "\n",
    "meta_features_test = np.concatenate(\n",
    "    [meta_test_base, arima_test, lstm_test],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"meta_features_test shape:\", meta_features_test.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 10) EVALUATE META-MODEL ON Trend_8D (NO SMOOTHING)\n",
    "# ===============================================================\n",
    "y_proba_meta = meta_clf.predict_proba(meta_features_test)[:, 1]\n",
    "y_pred_meta  = (y_proba_meta >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n=========== META-MODEL PERFORMANCE on Trend_8D (raw) ===========\")\n",
    "print(f\"Accuracy = {accuracy_score(y_test, y_pred_meta):.4f}\")\n",
    "print(f\"F1       = {f1_score(y_test, y_pred_meta):.4f}\")\n",
    "print(f\"ROC-AUC  = {roc_auc_score(y_test, y_proba_meta):.4f}\")\n",
    "\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_meta))\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred_meta, digits=4))\n",
    "\n",
    "# ===============================================================\n",
    "# 11) SMOOTHED PREDICTION: USE LAST 8 DAYS MODEL PROBABILITIES\n",
    "#     forecast at T based on model probs from T-8..T-1\n",
    "# ===============================================================\n",
    "proba_series = pd.Series(y_proba_meta, index=X_test.index)\n",
    "\n",
    "# Rolling mean over past 8 predictions, aligned so that\n",
    "# proba_smooth(T) uses predictions from T-8..T-1\n",
    "proba_smooth = proba_series.rolling(window=8).mean().shift(1)\n",
    "\n",
    "y_pred_smooth = (proba_smooth >= 0.5).astype(int)\n",
    "\n",
    "# Align with y_test (drop NaNs from smoothing)\n",
    "valid_idx = proba_smooth.dropna().index\n",
    "y_test_smooth = y_test.loc[valid_idx]\n",
    "y_pred_smooth_valid = y_pred_smooth.loc[valid_idx]\n",
    "\n",
    "print(\"\\n=========== META-MODEL PERFORMANCE with 8-day SMOOTHED forecast ===========\")\n",
    "print(f\"Accuracy = {accuracy_score(y_test_smooth, y_pred_smooth_valid):.4f}\")\n",
    "print(f\"F1       = {f1_score(y_test_smooth, y_pred_smooth_valid):.4f}\")\n",
    "print(f\"ROC-AUC  = {roc_auc_score(y_test_smooth, proba_smooth.loc[valid_idx]):.4f}\")\n",
    "\n",
    "print(\"\\nConfusion matrix (smoothed):\")\n",
    "print(confusion_matrix(y_test_smooth, y_pred_smooth_valid))\n",
    "\n",
    "print(\"\\nClassification report (smoothed):\")\n",
    "print(classification_report(y_test_smooth, y_pred_smooth_valid, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aadfc9-54a1-4220-a64f-b424cd2df916",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
