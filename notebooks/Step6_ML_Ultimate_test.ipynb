{
 "cells": [
  {
   "cell_type": "raw",
   "id": "db24864f-ff4f-42c9-bafd-cb780aa055da",
   "metadata": {},
   "source": [
    "# ===============================================================\n",
    "# MACHINE LEARNING PROJECT - 2025 GUIDELINES COMPLIANT\n",
    "# Title: Stock Price Direction Forecasting using Macroeconomic Indicators\n",
    "# Author: Dax AU\n",
    "# ===============================================================\n",
    "\n",
    "# Stock Price Direction Forecasting using Macroeconomic Indicators\n",
    "# -To predict whether a stock’s next-day return will be positive or negative, based on historical returns and macro-economic variables.\n",
    "\n",
    "# Predicting Apple Stock Direction — Full ML Pipeline\n",
    "# This notebook covers these sequential steps:\n",
    "\n",
    "9) Evaluate metrics & compare models\n",
    "10) PCA and overfitting checks\n",
    "11) Final comparison & conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4f0694cd-7df9-4cab-950a-a305dc9b4b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 1) IMPORT LIBRARIES\n",
    "# ===============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report, RocCurveDisplay\n",
    "import shap\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6e939bfd-5e2f-4da7-b5ca-4fac503faaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\data\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# 1) LOAD DATASETS\n",
    "# ===============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "OUT = Path(\"../data\")\n",
    "print(OUT)\n",
    "\n",
    "data = pd.read_csv(OUT / 'Cleaned_Features_for_ML.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "DATASET_LABEL = \"Cleaned_Features_for_ML\"   # or \"Cleaned_Features_for_ML_20ANOVA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0085b511-84c6-4abf-9314-a377698e50b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 4085 entries, 2010-03-15 to 2025-11-26\n",
      "Data columns (total 44 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   S&P500                             4085 non-null   float64\n",
      " 1   NASDAQ                             4085 non-null   float64\n",
      " 2   DowJones                           4085 non-null   float64\n",
      " 3   CAC40                              4085 non-null   float64\n",
      " 4   DAX                                4085 non-null   float64\n",
      " 5   FTSE100                            4085 non-null   float64\n",
      " 6   Nikkei225                          4085 non-null   float64\n",
      " 7   HangSeng                           4085 non-null   float64\n",
      " 8   MSCIWorld                          4085 non-null   float64\n",
      " 9   US10Y                              4085 non-null   float64\n",
      " 10  US2Y                               4085 non-null   float64\n",
      " 11  TLT                                4085 non-null   float64\n",
      " 12  IEF                                4085 non-null   float64\n",
      " 13  BND                                4085 non-null   float64\n",
      " 14  LQD                                4085 non-null   float64\n",
      " 15  Apple                              4085 non-null   float64\n",
      " 16  Microsoft                          4085 non-null   float64\n",
      " 17  Google                             4085 non-null   float64\n",
      " 18  Amazon                             4085 non-null   float64\n",
      " 19  Meta                               4085 non-null   float64\n",
      " 20  Inflation_CPI                      4085 non-null   float64\n",
      " 21  Unemployment_Rate                  4085 non-null   float64\n",
      " 22  Fed_Funds_Rate                     4085 non-null   float64\n",
      " 23  Real_GDP                           4085 non-null   float64\n",
      " 24  Yield_Curve_Spread                 4085 non-null   float64\n",
      " 25  Industrial_Production              4085 non-null   float64\n",
      " 26  M2_Money_Supply                    4085 non-null   float64\n",
      " 27  Personal_Consumption_Expenditures  4085 non-null   float64\n",
      " 28  Recession_Probability              4085 non-null   float64\n",
      " 29  GDP_Current_USD                    4085 non-null   float64\n",
      " 30  Inflation_Annual_Pct               4085 non-null   float64\n",
      " 31  Unemployment_Total_Pct             4085 non-null   float64\n",
      " 32  Exports_GDP_Pct                    4085 non-null   float64\n",
      " 33  Imports_GDP_Pct                    4085 non-null   float64\n",
      " 34  Public_Debt_GDP_Pct                4085 non-null   float64\n",
      " 35  OECD_CPI_2015idx_USA               4085 non-null   float64\n",
      " 36  OECD_Unemp_rate_pct_USA            4085 non-null   float64\n",
      " 37  Return                             4085 non-null   float64\n",
      " 38  Volatility_20d                     4085 non-null   float64\n",
      " 39  MA20                               4085 non-null   float64\n",
      " 40  MA50                               4085 non-null   float64\n",
      " 41  Momentum                           4085 non-null   float64\n",
      " 42  RSI                                4085 non-null   float64\n",
      " 43  Direction                          4085 non-null   int64  \n",
      "dtypes: float64(43), int64(1)\n",
      "memory usage: 1.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f41b0628-b394-4259-b113-8c13cc706b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  SAVE RESULTS HELPERS (JSON + CSV EXPORT)\n",
    "# ===============================================================\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ===============================================================\n",
    "# Define output folder inside project: /data\n",
    "# ===============================================================\n",
    "\n",
    "OUT = Path(\"../data\")   # <-- for notebooks located inside /notebooks/\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def make_json_serializable(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert objects (numpy arrays, numpy numbers, dicts, lists)\n",
    "    into JSON-serializable Python native types.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: make_json_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [make_json_serializable(i) for i in obj]\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.bool_, bool)):\n",
    "        return bool(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# ===============================================================\n",
    "# JSON + CSV Saving Utilities\n",
    "# ===============================================================\n",
    "\n",
    "def save_results_to_json(results_dict, filename=\"model_results.json\"):\n",
    "    \"\"\"Save the entire results dictionary into /data as JSON.\"\"\"\n",
    "    results_serializable = make_json_serializable(results_dict)\n",
    "    filepath = OUT / filename\n",
    "\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(results_serializable, f, indent=4)\n",
    "\n",
    "    print(f\"[INFO] Saved JSON to: {filepath.resolve()}\")\n",
    "\n",
    "\n",
    "def save_results_to_csv(results_dict, filename=\"model_results.csv\"):\n",
    "    \"\"\"Flatten model metrics into tabular CSV saved under /data.\"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for model_name, res in results_dict.items():\n",
    "        cm = np.array(res.get(\"confusion_matrix\"))\n",
    "        cr = res.get(\"classification_report\", {})\n",
    "        roc_auc = res.get(\"roc_auc\", None)\n",
    "        f2 = res.get(\"f2_score\", None)\n",
    "        comp_time = res.get(\"computation_time_sec\", None)\n",
    "\n",
    "        # Accuracy\n",
    "        acc = cr.get(\"accuracy\", None)\n",
    "\n",
    "        # Positive class key\n",
    "        pos_key = \"1\" if \"1\" in cr else None\n",
    "        if not pos_key:\n",
    "            keys = [k for k in cr.keys() if k.isdigit()]\n",
    "            if keys:\n",
    "                pos_key = keys[-1]\n",
    "\n",
    "        precision_1 = recall_1 = f1_1 = None\n",
    "        if pos_key and isinstance(cr.get(pos_key), dict):\n",
    "            precision_1 = cr[pos_key].get(\"precision\")\n",
    "            recall_1 = cr[pos_key].get(\"recall\")\n",
    "            f1_1 = cr[pos_key].get(\"f1-score\")\n",
    "\n",
    "        # Confusion matrix extract\n",
    "        tn = fp = fn = tp = None\n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        rows.append({\n",
    "            \"Dataset\": res.get(\"dataset_label\", \"Unknown\"),   # <--- added\n",
    "            \"Model\": model_name,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Precision (class 1)\": precision_1,\n",
    "            \"Recall (class 1)\": recall_1,\n",
    "            \"F1-score (class 1)\": f1_1,\n",
    "            \"F2-score\": f2,\n",
    "            \"ROC-AUC\": roc_auc,\n",
    "            \"Computation Time (sec)\": comp_time,\n",
    "            \"TN\": tn,\n",
    "            \"FP\": fp,\n",
    "            \"FN\": fn,\n",
    "            \"TP\": tp,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    filepath = OUT / filename\n",
    "    df.to_csv(filepath, index=False)\n",
    "\n",
    "    print(f\"[INFO] Saved CSV to: {filepath.resolve()}\")\n",
    "\n",
    "\n",
    "# GLOBAL CONTAINER\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cba68a1b-bb37-4c3b-9804-bacb39288e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(model_name, y_true, y_pred, y_prob=None, comp_time=None):\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, fbeta_score\n",
    "\n",
    "    # Create entry\n",
    "    results[model_name] = {}\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    results[model_name][\"confusion_matrix\"] = cm\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    results[model_name][\"classification_report\"] = report\n",
    "\n",
    "    # ROC-AUC\n",
    "    if y_prob is not None:\n",
    "        results[model_name][\"roc_auc\"] = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "    # F2-score\n",
    "    results[model_name][\"f2_score\"] = fbeta_score(y_true, y_pred, beta=2)\n",
    "\n",
    "    # Computation time\n",
    "    results[model_name][\"computation_time_sec\"] = comp_time\n",
    "\n",
    "    print(f\"✓ Saved model results for: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "69c9626d-37a0-445a-b93e-48d7d7abd370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after building Trend_8D target: (2969, 47)\n",
      "Fitting ARIMA(2,1,2) on Return...\n",
      "Preparing LSTM sequences...\n",
      "LSTM input shape: (2949, 20, 1)\n",
      "Training LSTM...\n",
      "Epoch 1/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 34ms/step - loss: 1.0087 - val_loss: 1.4584\n",
      "Epoch 2/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 1.0084 - val_loss: 1.4468\n",
      "Epoch 3/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1.0065 - val_loss: 1.4469\n",
      "Epoch 4/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1.0059 - val_loss: 1.4429\n",
      "Epoch 5/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1.0061 - val_loss: 1.4424\n",
      "Epoch 6/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1.0059 - val_loss: 1.4432\n",
      "Epoch 7/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1.0063 - val_loss: 1.4427\n",
      "Epoch 8/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1.0061 - val_loss: 1.4467\n",
      "Epoch 9/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1.0055 - val_loss: 1.4437\n",
      "Epoch 10/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1.0063 - val_loss: 1.4401\n",
      "Epoch 11/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1.0044 - val_loss: 1.4396\n",
      "Epoch 12/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1.0032 - val_loss: 1.4424\n",
      "Epoch 13/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1.0012 - val_loss: 1.4362\n",
      "Epoch 14/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1.0000 - val_loss: 1.4463\n",
      "Epoch 15/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.9964 - val_loss: 1.4361\n",
      "Epoch 16/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.9951 - val_loss: 1.4635\n",
      "Epoch 17/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.9924 - val_loss: 1.4549\n",
      "Epoch 18/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.9900 - val_loss: 1.4620\n",
      "Epoch 19/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.9879 - val_loss: 1.4552\n",
      "Epoch 20/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.9856 - val_loss: 1.4661\n",
      "Epoch 21/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.9832 - val_loss: 1.4589\n",
      "Epoch 22/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.9810 - val_loss: 1.4776\n",
      "Epoch 23/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.9813 - val_loss: 1.4509\n",
      "Epoch 24/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.9764 - val_loss: 1.4643\n",
      "Epoch 25/50\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.9739 - val_loss: 1.4658\n",
      "Forecasting with LSTM...\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "Data shape after ARIMA+LSTM dropna: (2948, 50)\n",
      "Base feature count (excluding ARIMA/LSTM): 44\n",
      "\n",
      "Building out-of-fold meta-features for Trend_8D...\n",
      "\n",
      "  LR OOF predictions...\n",
      "  KNN OOF predictions...\n",
      "  CART OOF predictions...\n",
      "  SVC OOF predictions...\n",
      "  MLP OOF predictions...\n",
      "  ABR OOF predictions...\n",
      "  GBR OOF predictions...\n",
      "  RFR OOF predictions...\n",
      "  ETR OOF predictions...\n",
      "meta_features_train shape: (2358, 11)\n",
      "\n",
      "Building meta-features for TEST...\n",
      "\n",
      "  LR full-train → test prediction...\n",
      "  KNN full-train → test prediction...\n",
      "  CART full-train → test prediction...\n",
      "  SVC full-train → test prediction...\n",
      "  MLP full-train → test prediction...\n",
      "  ABR full-train → test prediction...\n",
      "  GBR full-train → test prediction...\n",
      "  RFR full-train → test prediction...\n",
      "  ETR full-train → test prediction...\n",
      "meta_features_test shape: (590, 11)\n",
      "\n",
      "========= META-MODEL PERFORMANCE (8-DAY SMOOTHED) =========\n",
      "Accuracy = 0.7045\n",
      "F1       = 0.7448\n",
      "F2       = 0.7318\n",
      "ROC-AUC  = 0.7829\n",
      "\n",
      "Confusion Matrix (Smoothed):\n",
      " [[159  76]\n",
      " [ 96 251]]\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# 0) IMPORTS\n",
    "# ===============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier, GradientBoostingClassifier,\n",
    "    RandomForestClassifier, ExtraTreesClassifier\n",
    ")\n",
    "\n",
    "# Deep learning (LSTM)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# ===============================================================\n",
    "# 1) PREP DATA + CREATE 8-DAY FORWARD TREND TARGET\n",
    "# ===============================================================\n",
    "data = data.sort_index()\n",
    "\n",
    "required_cols = [\"Apple\", \"Return\", \"Direction\"]\n",
    "for c in required_cols:\n",
    "    if c not in data.columns:\n",
    "        raise ValueError(f\"Missing column: {c}\")\n",
    "\n",
    "# Forward-looking 8-day average of daily Direction\n",
    "# Direction_Forward8(t) = mean(Direction(t+1..t+8))\n",
    "data[\"Direction_Forward8\"] = (\n",
    "    data[\"Direction\"]\n",
    "    .rolling(8)\n",
    "    .mean()\n",
    "    .shift(-8)\n",
    ")\n",
    "\n",
    "# Binary trend label:\n",
    "#  1 if > 60% of future 8 days are up\n",
    "#  0 if < 40% of future 8 days are up\n",
    "#  NaN in between (neutral, we drop them for now)\n",
    "data[\"Trend_8D\"] = np.where(\n",
    "    data[\"Direction_Forward8\"] >= 0.6, 1,\n",
    "    np.where(data[\"Direction_Forward8\"] <= 0.4, 0, np.nan)\n",
    ")\n",
    "\n",
    "# Drop rows where target is NaN (start and end)\n",
    "data = data.dropna(subset=[\"Trend_8D\"]).copy()\n",
    "data[\"Trend_8D\"] = data[\"Trend_8D\"].astype(int)\n",
    "\n",
    "print(\"Data shape after building Trend_8D target:\", data.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 2) ARIMA FORECAST FEATURE (ON RETURN)\n",
    "# ===============================================================\n",
    "print(\"Fitting ARIMA(2,1,2) on Return...\")\n",
    "\n",
    "arima_model = ARIMA(data[\"Return\"], order=(2, 1, 2))\n",
    "arima_fit = arima_model.fit()\n",
    "\n",
    "arima_pred = arima_fit.predict(start=1, end=len(data), dynamic=False)\n",
    "arima_pred.index = data.index\n",
    "\n",
    "# Shift one step to avoid look-ahead\n",
    "data[\"ARIMA_Return_Forecast\"] = arima_pred.shift(1)\n",
    "data[\"ARIMA_Direction\"] = (data[\"ARIMA_Return_Forecast\"] > 0).astype(int)\n",
    "\n",
    "# ===============================================================\n",
    "# 3) LSTM FORECAST FEATURE (ON RETURN)\n",
    "# ===============================================================\n",
    "print(\"Preparing LSTM sequences...\")\n",
    "\n",
    "SEQ_LEN = 20\n",
    "returns_arr = data[\"Return\"].values.reshape(-1, 1)\n",
    "\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(SEQ_LEN, len(returns_arr)):\n",
    "    X_seq.append(returns_arr[i-SEQ_LEN:i])\n",
    "    y_seq.append(returns_arr[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "print(\"LSTM input shape:\", X_seq.shape)\n",
    "\n",
    "split_idx = int(0.8 * len(X_seq))\n",
    "X_train_seq, X_test_seq = X_seq[:split_idx], X_seq[split_idx:]\n",
    "y_train_seq, y_test_seq = y_seq[:split_idx], y_seq[split_idx:]\n",
    "\n",
    "model_lstm = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(SEQ_LEN, 1)),\n",
    "    LSTM(32),\n",
    "    Dense(1)\n",
    "])\n",
    "model_lstm.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "print(\"Training LSTM...\")\n",
    "model_lstm.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_split=0.1,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Forecasting with LSTM...\")\n",
    "lstm_pred = model_lstm.predict(X_seq).flatten()\n",
    "lstm_series = pd.Series(lstm_pred, index=data.index[SEQ_LEN:])\n",
    "\n",
    "# Shift by 1 to avoid look-ahead\n",
    "data[\"LSTM_Return_Forecast\"] = lstm_series.shift(1)\n",
    "data[\"LSTM_Direction\"] = (data[\"LSTM_Return_Forecast\"] > 0).astype(int)\n",
    "\n",
    "# ===============================================================\n",
    "# 4) DROP NaNs (from ARIMA/LSTM shifts and sequence start)\n",
    "# ===============================================================\n",
    "data_ml = data.dropna(subset=[\"ARIMA_Return_Forecast\", \"LSTM_Return_Forecast\"]).copy()\n",
    "print(\"Data shape after ARIMA+LSTM dropna:\", data_ml.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 5) FEATURES & TARGET FOR STACKING (Option C)\n",
    "# ===============================================================\n",
    "# Use ALL available predictors except:\n",
    "#  - raw price/return\n",
    "#  - daily Direction\n",
    "#  - forward-label helpers\n",
    "X_full = data_ml.drop(columns=[\n",
    "    \"Apple\", \"Return\", \"Direction\", \"Direction_Forward8\"\n",
    "])\n",
    "y_full = data_ml[\"Trend_8D\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# Keep ARIMA/LSTM forecasts separate for meta-layer\n",
    "arima_train = X_train[\"ARIMA_Return_Forecast\"].values.reshape(-1, 1)\n",
    "lstm_train  = X_train[\"LSTM_Return_Forecast\"].values.reshape(-1, 1)\n",
    "arima_test  = X_test[\"ARIMA_Return_Forecast\"].values.reshape(-1, 1)\n",
    "lstm_test   = X_test[\"LSTM_Return_Forecast\"].values.reshape(-1, 1)\n",
    "\n",
    "# Base feature set excludes ARIMA/LSTM forecasts\n",
    "base_feature_cols = [\n",
    "    c for c in X_train.columns\n",
    "    if c not in [\"ARIMA_Return_Forecast\", \"LSTM_Return_Forecast\"]\n",
    "]\n",
    "X_train_base = X_train[base_feature_cols]\n",
    "X_test_base  = X_test[base_feature_cols]\n",
    "\n",
    "print(\"Base feature count (excluding ARIMA/LSTM):\", len(base_feature_cols))\n",
    "\n",
    "# ===============================================================\n",
    "# 6) DEFINE BASE CLASSIFICATION MODELS\n",
    "# ===============================================================\n",
    "base_models = [\n",
    "    ('LR',  LogisticRegression(max_iter=5000, random_state=RANDOM_STATE)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('CART', DecisionTreeClassifier(max_depth=6, random_state=RANDOM_STATE)),\n",
    "    ('SVC',  SVC(kernel='rbf', probability=True, random_state=RANDOM_STATE)),\n",
    "    ('MLP',  MLPClassifier(hidden_layer_sizes=(64, 32),\n",
    "                           max_iter=5000, random_state=RANDOM_STATE)),\n",
    "    ('ABR',  AdaBoostClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('GBR',  GradientBoostingClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('RFR',  RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ('ETR',  ExtraTreesClassifier(n_estimators=300, random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "# ===============================================================\n",
    "# 7) STACKING – BUILD OUT-OF-FOLD META-FEATURES (TRAIN)\n",
    "# ===============================================================\n",
    "print(\"\\nBuilding out-of-fold meta-features for Trend_8D...\\n\")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "n_train = X_train_base.shape[0]\n",
    "n_models = len(base_models)\n",
    "\n",
    "meta_train_base = np.full((n_train, n_models), np.nan)\n",
    "\n",
    "for m_idx, (name, model) in enumerate(base_models):\n",
    "    print(f\"  {name} OOF predictions...\")\n",
    "    oof_pred = np.full(n_train, np.nan)\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(tscv.split(X_train_base)):\n",
    "        X_tr, X_val = X_train_base.iloc[tr_idx], X_train_base.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        oof_pred[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Fill NaNs (early samples not in any validation fold)\n",
    "    if np.isnan(oof_pred).any():\n",
    "        model.fit(X_train_base, y_train)\n",
    "        full_pred = model.predict_proba(X_train_base)[:, 1]\n",
    "        oof_pred = np.where(np.isnan(oof_pred), full_pred, oof_pred)\n",
    "\n",
    "    meta_train_base[:, m_idx] = oof_pred\n",
    "\n",
    "# Append ARIMA & LSTM forecasts to meta-features\n",
    "meta_features_train = np.concatenate(\n",
    "    [meta_train_base, arima_train, lstm_train],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"meta_features_train shape:\", meta_features_train.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 8) TRAIN META-CLASSIFIER (LOGISTIC REGRESSION)\n",
    "# ===============================================================\n",
    "meta_clf = LogisticRegression(max_iter=5000, random_state=RANDOM_STATE)\n",
    "meta_clf.fit(meta_features_train, y_train)\n",
    "\n",
    "# ===============================================================\n",
    "# 9) META-FEATURES FOR TEST\n",
    "# ===============================================================\n",
    "print(\"\\nBuilding meta-features for TEST...\\n\")\n",
    "\n",
    "meta_test_base = np.zeros((X_test_base.shape[0], n_models))\n",
    "\n",
    "for m_idx, (name, model) in enumerate(base_models):\n",
    "    print(f\"  {name} full-train → test prediction...\")\n",
    "    model.fit(X_train_base, y_train)\n",
    "    meta_test_base[:, m_idx] = model.predict_proba(X_test_base)[:, 1]\n",
    "\n",
    "meta_features_test = np.concatenate(\n",
    "    [meta_test_base, arima_test, lstm_test],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"meta_features_test shape:\", meta_features_test.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 10) EVALUATE META-MODEL (RAW PREDICTION)\n",
    "# ===============================================================\n",
    "\n",
    "from time import time\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "#print(\"\\n========== META-MODEL PERFORMANCE on Trend_8D (RAW) ==========\")\n",
    "\n",
    "t0 = time()\n",
    "y_proba_meta = meta_clf.predict_proba(meta_features_test)[:, 1]\n",
    "y_pred_meta  = (y_proba_meta >= 0.5).astype(int)\n",
    "elapsed_raw = time() - t0\n",
    "\n",
    "acc_raw = accuracy_score(y_test, y_pred_meta)\n",
    "f1_raw  = f1_score(y_test, y_pred_meta)\n",
    "f2_raw  = fbeta_score(y_test, y_pred_meta, beta=2)\n",
    "auc_raw = roc_auc_score(y_test, y_proba_meta)\n",
    "\n",
    "cm_raw     = confusion_matrix(y_test, y_pred_meta)\n",
    "report_raw = classification_report(y_test, y_pred_meta, output_dict=True)\n",
    "\n",
    "\n",
    "#print(f\"Accuracy = {acc_raw:.4f}\")\n",
    "#print(f\"F1       = {f1_raw:.4f}\")\n",
    "#print(f\"F2       = {f2_raw:.4f}\")\n",
    "#print(f\"ROC-AUC  = {auc_raw:.4f}\")\n",
    "#print(\"\\nConfusion Matrix:\\n\", cm_raw)\n",
    "\n",
    "# ---- Save results for dashboard ----\n",
    "#results[\"Trend8D_MetaModel_Raw\"] = {\n",
    "#    \"dataset_label\": DATASET_LABEL,\n",
    "#    \"test_accuracy\": float(acc_raw),\n",
    "#    \"roc_auc\": float(auc_raw),\n",
    "#    \"f1_score\": float(f1_raw),\n",
    "#    \"f2_score\": float(f2_raw),\n",
    "#    \"computation_time_sec\": float(elapsed_raw),\n",
    "#    \"confusion_matrix\": cm_raw.tolist(),\n",
    "#    \"classification_report\": report_raw\n",
    "#}\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 11) SMOOTHED META-PREDICTION (8-DAY ROLLING)\n",
    "# ===============================================================\n",
    "\n",
    "print(\"\\n========= META-MODEL PERFORMANCE (8-DAY SMOOTHED) =========\")\n",
    "\n",
    "proba_series = pd.Series(y_proba_meta, index=X_test.index)\n",
    "proba_smooth = proba_series.rolling(window=8).mean().shift(1)\n",
    "\n",
    "# Drop NaN\n",
    "valid_idx = proba_smooth.dropna().index\n",
    "y_test_smooth = y_test.loc[valid_idx]\n",
    "y_pred_smooth = (proba_smooth.loc[valid_idx] >= 0.5).astype(int)\n",
    "\n",
    "t0 = time()\n",
    "acc_s = accuracy_score(y_test_smooth, y_pred_smooth)\n",
    "f1_s  = f1_score(y_test_smooth, y_pred_smooth)\n",
    "f2_s  = fbeta_score(y_test_smooth, y_pred_smooth, beta=2)\n",
    "auc_s = roc_auc_score(y_test_smooth, proba_smooth.loc[valid_idx])\n",
    "elapsed_smooth = time() - t0\n",
    "\n",
    "cm_s     = confusion_matrix(y_test_smooth, y_pred_smooth)\n",
    "report_s = classification_report(y_test_smooth, y_pred_smooth, output_dict=True)\n",
    "\n",
    "print(f\"Accuracy = {acc_s:.4f}\")\n",
    "print(f\"F1       = {f1_s:.4f}\")\n",
    "print(f\"F2       = {f2_s:.4f}\")\n",
    "print(f\"ROC-AUC  = {auc_s:.4f}\")\n",
    "print(\"\\nConfusion Matrix (Smoothed):\\n\", cm_s)\n",
    "\n",
    "# ---- Save results for dashboard ----\n",
    "results[\"Trend8D_MetaModel_Smoothed\"] = {\n",
    "    \"dataset_label\": DATASET_LABEL,\n",
    "    \"test_accuracy\": float(acc_s),\n",
    "    \"roc_auc\": float(auc_s),\n",
    "    \"f1_score\": float(f1_s),\n",
    "    \"f2_score\": float(f2_s),\n",
    "    \"computation_time_sec\": float(elapsed_smooth),\n",
    "    \"confusion_matrix\": cm_s.tolist(),\n",
    "    \"classification_report\": report_s\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "11aadfc9-54a1-4220-a64f-b424cd2df916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved JSON to: C:\\Users\\dax_a\\Documents\\GitHub\\ESILV-MLproject-AU-BEJOT\\data\\model_results.json\n",
      "[INFO] Saved CSV to: C:\\Users\\dax_a\\Documents\\GitHub\\ESILV-MLproject-AU-BEJOT\\data\\model_results.csv\n"
     ]
    }
   ],
   "source": [
    "save_results_to_json(results, filename=\"model_results.json\")\n",
    "save_results_to_csv(results,  filename=\"model_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb000c4-36f6-4d6f-b6e4-3e2833b96525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
